README
mRS90 LightGBM Pipeline & Clinical Calculator
1. Purpose and Scope
This document describes a machine learning pipeline to predict 90-day functional outcome after mechanical thrombectomy for ischemic stroke. The focus is on:
•	Outcome: Good functional outcome at 90 days, defined as modified Rankin Scale 0–2 (denoted mRS90 for mRS at 90 days).
•	Predictors: Pre-procedural variables only – demographics, baseline clinical measures, imaging findings, and pre-treatment timings/treatments. (No intra- or post-procedural data like TICI scores or complications are used in the primary model.)
•	Model: A LightGBM gradient-boosted trees classifier with carefully defined monotonicity constraints reflecting clinical knowledge (see Model Specification below).
•	Cohort: A single-center retrospective thrombectomy registry (patients detailed in mt_patients_master.xlsx).
The project is structured as a Kedro pipeline that:
•	Cleans the raw registry data and applies strict data-quality checks and exclusions.
•	Prepares a modeling dataset suitable for LightGBM, without imputation (missing values are handled via indicators and by the model’s internal mechanisms).
•	Splits the cohort temporally into a training (development) set and an internal test set (simulating prospective validation).
•	Trains and tunes a monotone-constrained LightGBM model to predict mRS90, including automatic hyperparameter optimization (Optuna).
•	Evaluates the model with extensive internal validation: cross-validation, discrimination, calibration (with logistic recalibration analysis), decision curve analysis for clinical utility, and explainability via SHAP.
•	Generates comprehensive reports and documentation following TRIPOD guidelines (e.g., baseline characteristics by outcome, missingness summary, model specification, predictor definitions).
•	Deploys the final model in a password-protected Streamlit web application that allows clinicians to input patient data and get a probabilistic outcome prediction with explanations.
Important: This tool is for research only. It is not a clinical decision support system and must not be used to guide individual patient care without further validation. The aim is to provide transparency and facilitate internal analysis; any real-world use would require external validation, prospective assessment, and regulatory approval.
2. High-Level Data Flow
Conceptually, the pipeline flows from raw data to final model and app as follows:
Raw registry (Excel: mt_patients_master.xlsx)
   │
   ▼
[Data Cleaning & Validation] **(data_cleaning pipeline)**
   - clean_mt_patients → mt_patients_clean.parquet  
   - exclude_invalid_cases → mt_patients_valid.parquet  
   - validate_and_fix → mt_patients_validated.parquet  
   │
   ├──► **EDA & Audit** (eda_audit pipeline) → data_profile.json, eda_audit_report.json, quality figures (NIHSS, mRS, TICI distributions)
   │
   ▼
Validated cohort (cleaned & quality-checked data)
   │
   └──► **Feature Engineering**  
        make_lightgbm_ready → **mt_lightgbm_ready.parquet** (LightGBM-ready dataset with missingness indicators)
        *(A separate logistic-regression dataset can be created but is not used in this LightGBM pipeline)*
   ▼
[Temporal Split] **(data_splitting pipeline)**  
   - create_temporal_split → mt_patients_split.parquet (adds `split=train/test` based on onset year)
   ▼
[LightGBM mRS90 Modeling] **(modeling_mrs90 pipeline)**  
   - build_mrs90_dataset → X_train, y_train, X_test, y_test, IDs, feature_list  
   - optuna_tune_mrs90_lgbm → **mrs90_lgbm_optuna_best_params**, CV metrics (10×20)[1][2]  
   - train_mrs90_lgbm (with best params) → **mrs90_lgbm_model.pkl**, train_metrics  
   - evaluate_mrs90_lgbm → test_metrics, test_predictions  
   - mrs90_feature_importance → feature_importance.parquet  
   - compute_mrs90_shap → raw SHAP values, SHAP summaries (overall & age-stratified)  
   - plot_mrs90_shap, plot_mrs90_shap_age_groups → SHAP global plots, age-stratified SHAP plots  
   ▼
[Reporting & TRIPOD] **(reporting_mrs90 pipeline)**  
   - build_cohort_flow → mt_cohort_flow_table (patient counts at each stage)  
   - plot_cohort_flow_diagram → mt_cohort_flow_diagram.png (CONSORT-style flow chart)  
   - mrs90_outcome_availability → mrs90_outcome_availability.csv (who has 90d mRS, overall/train/test)  
   - mrs90_missingness_table → mrs90_missingness_table.parquet (missing data summary for all features)  
   - make_baseline_by_outcome → mrs90_baseline_by_outcome.csv (train-set baseline vars by outcome 0–2 vs 3–6)  
   - build_predictor_dictionary → mrs90_predictor_dictionary.xlsx (data dictionary of predictors)  
   - build_cv_summary_table → mrs90_cv_summary_table.csv (aggregate CV performance)  
   - build_performance_summary → mrs90_performance_summary.csv (test metrics summary, raw vs calibrated)  
   - build_threshold_performance → mrs90_threshold_performance.csv (test sensitivity/specificity at threshold[s])  
   - build_model_spec → mrs90_model_spec.json (model parameters, features, constraints)  
   - build_shap_top30 → mrs90_shap_top30.parquet (top 30 features by |SHAP|)  
   - build_dca_summary → mrs90_dca_summary.csv (net benefit summary)  
   - build_tripod_excel → mrs90_tripod_excel.xlsx (all key results compiled for reporting)  
   ▼  
[Streamlit App] **(app_mrs90.py)** – uses final model and reporting outputs to provide an interactive clinician interface (with password protection).  
In summary, raw data is cleaned and validated, then turned into a modeling dataset. A temporally separated training set is used to tune and train a LightGBM model. The model is evaluated thoroughly, and results (tables, metrics, figures) are produced for interpretation and reporting. Finally, a Streamlit app serves the model for individual patient predictions with explanation and performance info.
3. Project Structure
Only the most relevant files and folders are highlighted below (the structure is based on Kedro conventions):
•	conf/ – Configuration files
•	base/ – Base configuration (committed)
o	catalog.yml – Catalog of all data inputs/outputs and their file paths (Kedro DataSets)
o	column_map.yml – Mapping of raw Excel column names to standardized names.
o	parameters.yml – (Currently not heavily used; reserved for tunable parameters)
o	value_maps.yml – Definitions for translating raw categorical values (e.g., language translations, yes/no encodings).
•	local/ – Local configuration (not committed)
o	credentials.yml – Secrets or credentials (e.g., for the Streamlit app password)
o	(Also logging.yml for logging config and a README.md specific to config usage)
•	data/ – Data sets tracked by Kedro (not all included in repo due to size/privacy)
•	01_raw/ – Raw data (input)
o	mt_patients_master.xlsx – Original thrombectomy registry export (contains all patient data pre-cleaning).
•	02_intermediate/ – Intermediate data (after initial cleaning)
o	mt_patients_clean.parquet – Cleaned data (normalized and translated, but prior to exclusions).
•	03_primary/ – Primary data sets (after exclusions and validation)
o	mt_patients_valid.parquet – Data after dropping invalid records (hard exclusions).
o	mt_patients_validated.parquet – Data after value range checks/fixes (no new exclusions, but values adjusted or blanked as needed).
o	mt_patients_split.parquet – Same as validated data but with an added split column (train/test assignment per temporal rule).
•	04_feature/ – (Unused in this project; reserved for feature-engineering outputs in other projects.)
•	05_model_input/ – Final input data for modeling
o	mt_lightgbm_ready.parquet – Main modeling table for LightGBM (post-cleaning, with engineered features and missingness indicators, no imputation).
o	(Optionally mt_patients_regression_ready.parquet would live here if logistic regression analysis was performed, but that pipeline is deprecated.)
•	06_models/ – Stored trained model artifacts
o	modeling_mrs90/ – Directory for all mRS90 LightGBM model artifacts
o	mrs90_X_train.parquet, mrs90_y_train.parquet – Training set features and labels (post-split).
o	mrs90_X_test.parquet, mrs90_y_test.parquet – Test set features and labels.
o	mrs90_train_ids.parquet, mrs90_test_ids.parquet – Lists of patient IDs in train and test sets.
o	mrs90_feature_list.json – List of feature names used in modeling (columns of X matrices).
o	mrs90_lgbm_model.pkl – Serialized final LightGBM model object.
o	mrs90_lgbm_optuna_best_params.json – Best hyperparameters found via Optuna tuning (including flags for interactions, etc.).
•	07_model_output/ – (Reserved for model predictions or outputs; not heavily used here since outputs are in 08_reporting)
•	08_reporting/ – All analysis outputs, reports, and figures for review/reporting
o	baseline_characteristics/ – Overall cohort baseline comparison between Train vs Test
o	baseline_table1.csv – “Table 1” style summary of baseline variables in training vs test set (mean±SD or counts, plus SMD).
o	baseline_smd.csv – Simplified table of standardized mean differences for each variable (sorted by imbalance).
o	data_cleaning/ – Logs from data cleaning pipeline
o	cleaning_log.json – Detailed log of cleaning actions (renaming, coercions, translations, etc.).
o	exclusion_log.json – Summary of exclusions made (count of records removed for each rule, etc.).
o	data_quality_violations.json – Log of range or plausibility issues found and fixed (violations of ranges, etc.).
o	imputation/regression_imputation_log.json – (Log for regression imputation step, if it were used).
o	eda_audit/ – Outputs of the exploratory data analysis and audit pipeline
o	eda_audit_report.json – Summary report of the data audit (distribution of variables, outliers, etc.).
o	data_profile.json – Combined data profile (merging cleaning logs and EDA info for an overview of data quality).
o	figs/ – Directory of generated figures (matplotlib) for key distributions:
	e.g., nihss_7d_hist.png, mrs90_bar.png, tici_bar.png – Histograms or bar charts illustrating NIHSS, mRS90 outcome distribution, TICI scores, etc., for audit purposes.
o	modeling_mrs90/ – Outputs related to model performance and analysis
o	cv/ – Cross-validation metrics
	mrs90_lgbm_cv_metrics_raw_10x20.json – Metrics for 10×20 repeated stratified CV on the training set (each fold’s AUC, Brier, etc., plus summary stats).
	mrs90_cv_summary_table.csv – Tabular summary of CV results (mean±SD of metrics) for easy reference.
	mrs90_lgbm_cv_auc_distribution_raw.png – Histogram of AUCs across all 200 CV runs (visualizing performance stability).
o	calibration/ – Calibration analysis
	mrs90_calibration_metrics_raw.json – Calibration metrics on train vs test (AUC, Brier, log-loss, calibration intercept and slope) before any recalibration[3][4].
	mrs90_logistic_calibration_params.json – Parameters (slope and intercept) for logistic calibration (Platt scaling) derived from the training set.
	mrs90_calibration_curve_raw.png – Calibration curve plot (observed vs predicted probability in deciles) for the raw model on test[5].
	mrs90_calibration_curve_raw_vs_calibrated.png – Calibration curves comparing raw vs after logistic calibration (showing improvement in alignment to the 45° line).
o	decision_curve/ – Decision Curve Analysis for clinical utility
	mrs90_decision_curve_raw.csv – Net benefit values of the model across probability thresholds (raw model predictions).
	mrs90_decision_curve_raw.png – Plot of decision curves for the raw model vs “treat-all” vs “treat-none”[6].
	mrs90_decision_curve_calibrated.csv – Net benefit values if model outputs are calibrated (adjusted probabilities) before thresholding.
	mrs90_decision_curve_calibrated.png – Decision curve plot for the calibrated model.
	Clinically, comparing raw vs calibrated decision curves can show if calibration changes the range of thresholds where using the model adds value.
o	evaluate/ – Test set evaluation outputs
	mrs90_train_metrics.json – Training performance metrics (AUC, Brier score, log-loss on train, etc.).
	mrs90_test_metrics.json – Test set performance metrics (AUC, Brier, log-loss on held-out test).
	mrs90_test_predictions.parquet – Per-patient test set predictions (with true outcome, predicted probability, and predicted class label).
	mrs90_feature_importance.parquet – Feature importance scores from the LightGBM model (e.g., gain or split importance for each feature).
o	performance/ – Threshold-based performance metrics
	mrs90_performance_summary.csv – Summary of model discrimination and calibration on test (e.g. AUC, Brier, calibration slope/intercept for raw model, etc., possibly combining info from above).
	mrs90_threshold_performance.csv – Detailed performance at one or more probability thresholds (e.g., confusion matrix metrics like sensitivity, specificity, PPV at a chosen threshold like 0.5, potentially both raw and calibrated probabilities).
o	shap/ – SHAP (SHapley Additive exPlanations) outputs for model explainability
	mrs90_shap_train_values.pkl, mrs90_shap_test_values.pkl – Raw SHAP value arrays for all training and test set samples (pickle format, large).
	mrs90_shap_expected_value.json – The SHAP “expected value” of the model (base log-odds or probability for an average patient)[7].
	mrs90_shap_summary_train.parquet, mrs90_shap_summary_test.parquet – Precomputed summary of SHAP values by feature (mean, std, etc.) for train and test sets.
	mrs90_shap_summary_test_by_age.parquet – SHAP summary broken down by age groups (to examine how feature impacts differ in younger vs older patients).
	Global SHAP plots:
	mrs90_shap_barplot.png – Bar plot of top features by mean |SHAP| (global feature importance)[8].
	mrs90_shap_beeswarm.png – Beeswarm plot of SHAP values for all features (distribution of impacts)[8].
	mrs90_shap_dependence_top_feature.png – SHAP dependence plot for the highest-impact feature (effect vs feature value)[9].
	Age-stratified SHAP plots (comparing SHAP distributions in younger vs older patients for key age cutoffs):
	mrs90_shap_age_lt50_ge50.png, ...lt75_ge75.png, ...lt80_ge80.png, ...lt85_ge85.png – Beeswarm-like plots split by age group, highlighting interactions between age and other predictors.
	For clinicians, the SHAP outputs identify which variables most strongly drive the model’s predictions and how their influence may change with patient age[10].
o	spec/ – Model specification details
	mrs90_model_spec.json – Machine-readable model specification including model parameters, selected features, and any constraints (monotonicity signs), to document exactly how the model was built.
o	tripod/ – Compiled TRIPOD report outputs
	mrs90_tripod_excel.xlsx – An Excel workbook containing multiple sheets summarizing the entire modeling process and results (cohort flow, outcome availability, missingness, predictor dictionary, model performance, etc.) for reporting transparency.
o	reporting_mrs90/ – Final tabular outputs specific to mRS90 model reporting (also contained in the TRIPOD Excel above)
o	mt_cohort_flow_table.csv – Table of patient counts at each step of data processing (for CONSORT-style diagram and reporting).
o	mt_cohort_flow_diagram.png – Flow diagram illustrating the cohort selection (a visual representation of the above table).
o	mrs90_outcome_availability.csv – Counts and percentages of patients with 90-day mRS available, overall and by train/test split.
o	mrs90_missingness_table.parquet – Detailed table of missing data by variable (overall, in train, in test; flags for whether a variable has a companion “_missing” indicator, etc.).
o	mrs90_baseline_by_outcome.csv – Baseline characteristics of the training set, stratified by 90-day outcome (mRS 0–2 vs 3–6), including group means/counts and standardized mean differences for each predictor.
o	mrs90_predictor_dictionary.xlsx – Data dictionary for all predictors, including data type, allowed range or categories, whether a monotonic constraint is applied (and direction), whether a missing-value flag exists, and whether the predictor was actually used in the final model.
o	mrs90_cv_summary_table.csv – Summary of cross-validation performance (aggregate of the 10×20 CV results for the final model parameters).
o	mrs90_performance_summary.csv – Summary of final model performance on the internal test set (AUC, Brier, etc., and possibly calibration metrics).
o	mrs90_threshold_performance.csv – Performance metrics at specific probability threshold(s) on the test set (e.g., confusion matrix outcomes like sensitivity, specificity for a chosen operating point, helping to contextualize the model’s output clinically).
o	mrs90_dca_summary.csv – Summary of decision curve analysis results (e.g., the range of risk thresholds at which the model has positive net benefit and surpasses treat-all or treat-none strategies).
•	src/ – Project source code
•	mechanical_thrombectomy_outcome_predictor/ – Python package with the pipeline code
o	pipeline_registry.py – Registers all pipelines and specifies how they combine into the overall workflow (which pipelines run by default, etc.)[11].
o	settings.py – Kedro project settings (e.g., naming the default pipelines package).
o	pipelines/ – Directory containing individual pipeline definitions, each as a subpackage:
o	data_cleaning/ – Data cleaning & validation pipeline
	nodes.py – Functions (“nodes”) that perform cleaning steps (detailed in section 4.2 below).
	pipeline.py – Combines cleaning nodes into a Kedro Pipeline definition.
o	eda_audit/ – Exploratory Data Analysis pipeline
	nodes.py – Functions for profiling data and generating EDA reports/figures (e.g., distribution plots, data quality summary).
	pipeline.py – Pipeline definition for EDA (runs after data_cleaning on the validated data).
o	data_splitting/ – Temporal splitting pipeline
	nodes.py – Function to add the train/test split label based on onset date.
	pipeline.py – Pipeline definition for splitting (runs after validation).
o	baseline_characteristics/ – Baseline comparison pipeline (Train vs Test)
	nodes.py – Function to generate baseline tables comparing train and test cohorts (calculating means and SMDs)[12][13].
	pipeline.py – Pipeline definition (outputs baseline_table1 and baseline_smd).
o	modeling_mrs90/ – Model development pipeline for the mRS90 LightGBM model
	nodes.py – Core functions for dataset assembly, hyperparameter tuning, model training, evaluation, and SHAP analysis.
	pipeline.py – Pipeline definition listing each step of model development in order[1][2].
	config.py – Contains configuration for model features and monotone constraints (e.g., mapping of feature names to their expected monotonic direction).
	calibration_utils.py – Helper functions for calibration (e.g., computing calibration slope and intercept, applying logistic calibration to probabilities).
o	reporting_mrs90/ – Reporting pipeline (post-modeling analyses for TRIPOD)
	nodes.py – Functions to create cohort flow summaries, outcome availability, missingness tables, predictor dictionary, performance summaries, decision curve summaries, and to aggregate all results into the TRIPOD Excel.
	pipeline.py – Pipeline definition chaining the reporting nodes (runs after the modeling_mrs90 pipeline to produce final report artifacts)[14][15].
o	Other files in src/ root:
o	__main__.py – Entry-point allowing the project to be run as a Python module.
o	(The project is structured so that running kedro run executes the __default__ pipeline which includes data_cleaning, data_splitting, baseline_characteristics, eda_audit, modeling_mrs90 pipelines in sequence[11]. The reporting_mrs90 pipeline can be run separately to generate the documentation outputs after the model is trained.)
•	tests/ – Unit tests
o	test_run.py – A basic test to execute a full pipeline run on a subset of data (or dummy data) to ensure nothing breaks.
•	calibration_mrs90_analysis.py, cv_mrs90_evaluation.py, decision_curve_mrs90.py – Standalone analysis scripts (outside Kedro pipelines) used for additional evaluation. For example, cv_mrs90_evaluation.py may run extensive cross-validation to validate hyperparameters, and calibration_mrs90_analysis.py computes detailed calibration plots (these scripts read from data/06_models and write to data/08_reporting). In the current version, much of their functionality has been integrated into the pipelines (Optuna tuning for CV, and pipeline nodes for decision curves, etc.), though the scripts can still be used or adapted for further analysis.
•	app_mrs90.py – Streamlit application script for deploying the model (detailed in section 10).
4. From Raw Data to LightGBM-Ready Table
4.1 Raw Data (01_raw)
mt_patients_master.xlsx – This Excel file contains the raw thrombectomy registry data exported from the hospital’s system. It includes all original columns as recorded clinically, which are a mix of Czech and English terms, various encodings for yes/no, and possibly inconsistent notations for missing values or categories. This is the starting point for the pipeline. (Patient identifiers are pseudonymized or removed; each row represents one thrombectomy case with numerous variables covering demographics, medical history, imaging findings, treatments, and outcomes.)
4.2 Data Cleaning & Validation Pipeline (data_cleaning)
Location: src/mechanical_thrombectomy_outcome_predictor/pipelines/data_cleaning/.
This pipeline is responsible for ingesting the raw Excel and producing a cleaned, standardized dataset, while logging any data quality issues. It consists of several nodes (functions) executed in sequence:
•	clean_mt_patients – Raw data normalization and translation.
Inputs:
– mt_patients_raw (the raw Excel data, read as a DataFrame)
– column_map (mapping of raw column names to standardized names from conf/base/column_map.yml)
– value_maps (dictionary of value translations from conf/base/value_maps.yml, e.g. mapping Czech words to English, or yes/no variants to 1/0)
Outputs:
– mt_patients_clean (DataFrame saved as parquet)
– cleaning_log.json (detailed log of actions and issues)
What it does:
- Normalize column names: Trims whitespace, converts to lowercase, replaces spaces/hyphens with underscores (so all columns become snake_case)[16].
- Rename columns to standard names: Uses column_map.yml to rename columns to a consistent set of clinical variables (e.g., “pohlaví” → “sex”, “vek” → “age”, etc.)[17]. Any expected columns that are missing after renaming are noted in the log, as well as any raw columns that were not mapped (extra or deprecated fields)[18].
- Translate values: Applies the mappings in value_maps.yml to replace certain categorical values. For example, it can translate Czech words to English (“Ano” → “Yes”), unify different representations of yes/no, and normalize categorical encodings (e.g., convert text codes into numeric or standardized text)[19].
- Trim whitespace and standardize missing markers: Strips leading/trailing whitespace from all cell values and converts various placeholders to actual missing (NaN/NA)[20][21]. For instance, empty strings, “NA”, “N/A”, “?“, and “none” (in any case) are all interpreted as missing data[22]. (Notably, the word “No finding” in clinical context is kept as a valid category, not treated as missing.)
- Normalize key categorical variables: For example, the TICI reperfusion grade is recorded in various formats; the cleaning code standardizes it to an ordered categorical with allowed values {0, 1, 2a, 2b, 2c, 3}[23][24]. It handles common quirks like “IIb” or “2b/3” and maps them to the nearest valid category (e.g., “2b/3” → “2b”, assuming partial recanalization)[24]. This ensures TICI is analyzable and ordinal.
- Coerce numeric types: Many numeric fields come in as text; the pipeline attempts to convert important ones to numeric dtype (int or float). Fields like age, NIHSS scores, ASPECTS, blood pressure, glucose, cholesterol, time intervals, etc., are force-converted to numeric, with any non-conformant entries becoming NaN[25]. The number of values coerced to NaN for each field is logged to cleaning_log.json, so we know if, say, “NIHSS 24h” had some non-numeric entries that got dropped[26][27].
- Standardize binary fields: Many yes/no or boolean fields are normalized to 0/1. The value_maps cover some, and a helper (_to_binary) maps known textual tokens to True/False/NA consistently[28]. For example, something recorded as “Y” or “yes” becomes 1, “N” or “no” becomes 0. After this, binary columns are cast to a compact integer type (Int8) for efficiency[29].
- Composite variable handling: For instance, the registry might have a field for heart rhythm diagnoses. In our model we only care if any heart condition was present. The cleaning step collapses a multi-category heart_condition variable into a binary: 0 = “no known cardiac pathology”, 1 = “some heart condition present”[30]. This way, rare categories (AF, heart failure, etc.) are merged into a single risk factor flag, simplifying the model and increasing statistical power.
- Intra-field consistency: Enforces simple logical rules. E.g., if a patient is recorded as not receiving IV thrombolysis (ivt_given = 0), then any recorded value for “onset_to_ivt_min” (onset-to-IVT time) is meaningless and is set to NA[31]. This prevents impossible combinations from propagating. It also logs the proportion of cases where ivt_given=1 but the corresponding time is missing, as a data quality indicator[32].
- Logging: The cleaning_log.json captures all the above actions – how many columns were renamed, how many values translated, counts of missing values before/after, any irregularities encountered, etc. This provides transparency (TRIPOD compliance) that the dataset was processed in a deterministic, documented way.
Clinical perspective: After this step, the dataset’s variables are standardized and human-readable. Obvious data entry issues (like “NA” vs “No”) are resolved. Clinicians reviewing the data can now see variables in familiar terms (English, consistent units), and all bizarre or non-numeric entries have been flagged or stripped. It transforms the hospital registry into a cleaner dataset ready for analysis.
•	exclude_invalid_cases – Hard exclusions for implausible data.
Input: mt_patients_clean (output of previous node)
Outputs:
– mt_patients_valid (DataFrame after dropping invalid records)
– exclusion_log.json (log of how many records were removed for each rule)
What it does: This node drops patient records that violate fundamental physiological or logical constraints – essentially data points considered too flawed to use. Criteria include[33][34]:
- Impossible time sequences: If “onset to IVT time” > “onset to puncture time”, or “onset to puncture” > “onset to recanalization”, those records are inconsistent (treatment times out of order) and are removed[34][35].
- Out-of-range clinical scores: If ASPECTS (Alberta Stroke Program Early CT Score) is > 10 (outside 0–10 scale), drop the record[36]. If any NIHSS (National Institutes of Health Stroke Scale) score is > 42 (beyond the standard 0–42 range for NIHSS), drop the record[37]. These likely indicate data errors (e.g., extra digit or wrong unit).
- Other possible hard exclusions can include impossible dates (if present) or duplicate entries (if identified, though not explicitly listed here).
The exclusion_log.json tallies how many records were removed for each reason (e.g., X patients removed for NIHSS >42, Y for timeline inconsistencies) and the total remaining records[38]. After this step, the cohort (mt_patients_valid) contains only cases that passed basic plausibility checks.
Clinical perspective: This exclusion step ensures that the model is not trained on data that are clearly erroneous (for example, a recorded NIHSS of 99, or a case where procedure time precedes stroke onset – which could happen if dates were entered incorrectly). By removing such cases, we focus the model on patients that could exist in reality, improving reliability. For a clinician, it’s reassuring that the developers have removed nonsensical data rather than letting them distort the model. The number of exclusions is also important – if many cases were dropped, that might indicate systemic data issues or a very strict quality threshold. Here, only a small fraction are typically removed.
•	validate_and_fix – Soft validation and data fixing for ranges.
Input: mt_patients_valid (from above, after exclusions)
Outputs:
– mt_patients_validated (DataFrame with some values adjusted or blanked, but no further row deletions)
– data_quality_violations.json (log of values that were out-of-range and how they were handled)
What it does: Instead of further dropping patients, this step retains all cases but addresses remaining data quality issues by modifying or flagging implausible values:
- Temporal consistency checks (logging only): It double-checks the monotonic time assumptions on the remaining data and logs any violations where times are still inconsistent (there shouldn’t be any after exclude_invalid_cases, but the code accounts for it)[39][40]. If found, it blanks the later time to maintain consistency (e.g., if onset_to_puncture < onset_to_ivt, it will set onset_to_puncture to NA)[41]. The indices of any such rows and counts are recorded in time_violations in the log.
- ASPECTS range correction: Any ASPECTS values outside 0–10 are set to NA (since these are considered measurement errors)[42]. The log records how many values were blanked for being out of range.
- mRS values validation: mRS (both baseline and 90-day) should be 0–6. Any value outside this (e.g., 9 or −1, sometimes used to denote missing in source) is set to NA. Afterward, mRS columns are explicitly cast to an integer type (nullable Int64) to ensure they are recognized as ordinal integers[43].
- NIHSS 7-day sanity check: While NIHSS is officially 0–42, in our data, values up to 99 appear, likely as placeholders for “patient deceased” or similar. We treat NIHSS scores >42 as impossible: values >99 are set to NA (they are considered absurd data entries), and values between 43 and 99 are not removed but flagged in the log as extreme out-of-range values[44]. (In practice, we see NIHSS=99 used when a 7-day score isn’t obtained because the patient died or was lost; we blank those out for analysis.)
- Binary field type casting: Ensures that all the yes/no fields that have been cleaned are stored as binary (Int8) types now. By this stage, fields like ivt_given, hypertension, etc., should already be 0/1 or NA; this just enforces the type and saves memory[45].
- Final integrity checks: The node may log counts of missing values per column and the final set of columns present[46], providing a “profile” of the cleaned dataset.
The data_quality_violations.json captures indices of any values that were adjusted. For example, it will list the row IDs where NIHSS7d was >42 (flagged) or >99 (blanked), where ASPECTS was out of range, etc., along with counts[47][44]. No patients are excluded here; instead, problematic values are set to missing so that the model can handle them appropriately (or they can be imputed for regression models if needed).
Clinical perspective: After this step, the dataset (mt_patients_validated) is considered analysis-ready. All remaining patients have plausible data for key fields. Extreme outliers have been nullified to avoid skewing analyses (for instance, an NIHSS of 99 is essentially treated as missing since it was a data code, not a real score). Clinicians can be confident that any obvious errors in data entry have either been removed or neutralized. Importantly, by not removing those patients entirely (unless absolutely necessary), we preserve sample size – we only dropped those cases that were beyond salvage (from the previous step). This approach aligns with TRIPOD recommendations for data cleaning: document what you assume or correct, and keep as much data as possible.
•	make_lightgbm_ready – Feature engineering for LightGBM.
Input: mt_patients_validated (cleaned DataFrame with no major violations)
Output: mt_lightgbm_ready (Parquet file for modeling)
What it does: This node takes the validated data and creates the final feature set needed for the machine learning model, performing minimal imputation but adding indicators so the model can handle missingness intelligently[48][49]:
- Derive onset_year: Extracts the year of stroke onset from the date-time (if available). This simple feature is crucial for the temporal split and also could capture secular trends in care over years[49]. If onset_date exists and onset_year isn’t already present, it’s added as an integer year column[49]. (This was done earlier in splitting too, but here we ensure the modeling data has onset_year explicitly as a feature if needed for temporal analysis.)
- Add missingness flags for predictors: For each key predictor, it creates a new binary feature <variable>_missing which is 1 if that variable is missing or NA, 0 otherwise[50][51]. Which variables get a flag? We focus on variables where missingness itself could carry information or where we don’t want to lose samples by dropping missing values.
- Numeric predictors: age, ASPECTS, onset-to-treatment times, NIHSS, blood pressure, BMI, cholesterol, glucose, etc. all get a _missing flag[50][51]. Even outcome mRS90 gets a flag, though for modeling we drop those with missing mRS90 anyway (so mRS90_missing flag is mostly for completeness or other analyses).
- Categorical predictors: sex, hemisphere (left/right), occlusion_site, stroke etiology, ivt_given, and many medical history flags (hypertension, diabetes, etc.) get missing flags as well[52][53]. Essentially, if any of these is not recorded, the _missing flag allows the model to know that and possibly adjust predictions.
- Why missing flags? LightGBM can internally handle missing values by routing them in the tree learning, but by providing an explicit missing indicator, we allow the model to learn if “unknown” is systematically associated with outcome. For example, if older patients tend not to have a certain lab measured (thus missing more often) and also tend to have worse outcomes, the model could otherwise infer something spurious. A missing indicator makes “missingness” a first-class feature. We do not do any filling/imputation at this stage – missing numeric values remain NaN (so the trees can split on them), and missing categoricals remain NaN (LightGBM treats them as a separate category implicitly). The added flags just give extra signal.
- No further cleaning or encoding beyond this. We retain all numeric values as is (except where flagged/blanked above) and categorical variables remain as either categories or objects that will later be cast to category in the modeling stage. The LightGBM algorithm will see, for example, a categorical occlusion site (M1, M2, etc.) as integers after we cast to category (done in the modeling pipeline), and missing flags for any that were NaN.
After this, mt_lightgbm_ready contains all the features needed for the model in a single table, along with patient_id and the outcome mRS90 (and its missing flag). No imputation is done – missing values are preserved – because tree-based models like LightGBM can handle them and often it’s better to let the model learn from the pattern of missingness rather than fill with potentially biasing defaults.
Clinical perspective: At this point, the data is in a form that a machine learning model can ingest directly. We’ve lost no patients except those with nonsense data, and we haven’t imputed anything, so we aren’t injecting any assumptions unwittingly. The addition of “missing” indicators might seem odd from a clinical standpoint, but it’s actually quite relevant: whether a value was measured can correlate with severity (for instance, maybe only sicker patients get a certain lab test – if it’s missing, perhaps the patient was well enough to not warrant it). By including these flags, the model can potentially use absence of data as a mild risk predictor, which in a clinical sense mimics a clinician’s intuition (“if they didn’t record X, it probably was normal”).
•	(Optional: make_regression_ready) – Imputation for logistic regression.
Input: mt_lightgbm_ready
Outputs:
– mt_patients_regression_ready.parquet (complete-case dataset)
– regression_imputation_log.json (log of imputation steps)
This step was part of an earlier analysis to build a logistic regression model for comparison. It performs mean/mode imputation (or a light MICE procedure) to fill all missing values, resulting in a dataset with no NaNs. However, the logistic regression pipeline (“mt_regression”) is deprecated in the current project version (we focus on LightGBM), so this node is not executed as part of the main pipeline. For completeness: it would use scikit-learn’s IterativeImputer (MICE) for numeric features and simple mode imputation for categoricals[54][55], tracking the before/after missing counts and ensuring *_missing flags remain to inform the regression model of originally missing entries[56][57]. This is mentioned in case one needs a traditional regression model, but our primary results use the non-imputed dataset with LightGBM.
After executing the data_cleaning pipeline, we have mt_patients_validated (clean data with all patients) and mt_lightgbm_ready (the features for modeling). We also have a suite of logs (cleaning_log.json, exclusion_log.json, etc.) that fully document how the raw data was transformed. This ensures transparency: any researcher can inspect those logs to see, for example, which patients were excluded and why, or how many values in each variable were blanked due to being out of range.
5. Temporal Train/Test Split (Internal Validation Design)
Pipeline: data_splitting – create_temporal_split
Input: mt_patients_validated (with onset_date and onset_time fields)
Output: mt_patients_split (same data but with an extra column split for train/test)
To evaluate the model in a way that mimics prospective performance, we perform a temporal split rather than a random split. The logic is simple[58][59]:
•	Define a cutoff year (by default, 2022 in our analysis). All patients with stroke onset year ≤ 2022 are assigned to “train”, and those with onset year ≥ 2023 go to “test.” This effectively uses earlier patients for model development and the most recent patients as a hold-out for testing.
•	If onset date is missing or not parseable for a patient, we conservatively assign that patient to the training set[60][61]. The reasoning is we don’t want to accidentally place an old patient into the test set (which is meant to simulate future data) – better to include ambiguous cases in training.
•	The function also adds the onset_year column if not present (as done in make_lightgbm_ready), and then sets a categorical column split with values “train” or “test.”
The result mt_patients_split.parquet is essentially the same table as mt_patients_validated but every row now knows if it’s designated for model training or for final testing. In our dataset, for example, strokes from 2018–2022 are training, and strokes from 2023 are test (assuming our data included up to 2023; if the registry is entirely ≤2022, then test set might be empty – but in our case there is a portion reserved as test).
Why temporal split? Clinically, this is a stronger internal validation. It prevents information leakages that could happen in random splits (where, say, the model might train on a 2021 patient and test on another 2021 patient – not realistic if we want to deploy a model on future patients). Temporal split simulates a scenario where you develop a model on past patients and apply it to new ones. It’s a tougher test, as medicine can change over time (outcomes might improve over years, etc.). If the model still performs well on a temporally later cohort, it’s more likely to be robust prospectively.
Additionally, by using time, we avoid any subtle leak (for instance, imagine if two rows were actually the same patient’s two procedures – a random split could put one procedure in train and one in test, giving an unfair advantage. A temporal split avoids that, under the assumption that re-treatments and longitudinal correlations would usually happen in chronological order).
6. mRS90 LightGBM Modeling Pipeline
6.1 Pipeline Overview and Data Set Assembly
Location: src/mechanical_thrombectomy_outcome_predictor/pipelines/modeling_mrs90/.
This pipeline takes the prepared data and develops the prognostic model for good 90-day outcome. It includes dataset construction, feature selection, hyperparameter tuning, model training, evaluation, and generating SHAP explainability outputs.
Key steps/nodes in this pipeline:
•	build_mrs90_dataset – Prepare modeling datasets (X/y for train and test)[62][63]
Inputs:
– mt_lightgbm_ready (full feature table with all patients)
– mt_patients_split (table with the split labels)
Outputs:
– mrs90_X_train, mrs90_y_train – Features and outcome for training set.
– mrs90_X_test, mrs90_y_test – Features and outcome for test set.
– mrs90_train_ids, mrs90_test_ids – Corresponding patient identifiers in each set (for reference).
– mrs90_feature_list – The list of feature names used.
What it does:
- Merge and filter: Joins mt_lightgbm_ready with the split labels on patient_id[64][65] to ensure each patient’s data knows whether it’s train or test.
- Exclude patients without outcome: We filter to only include patients where mRS90 is known (not NaN)[66]. If some patients did not have a 90-day outcome recorded, they cannot contribute to model training (since we don’t know the target). Those patients are essentially set aside (they could be used in an unsupervised way or for other outcomes, but not here). The number of such patients is captured in the cohort flow (Reporting pipeline will note how many patients were missing mRS90).
- Define the binary outcome: The node creates a binary label good_mrs (in code they use 1 for mRS 0–2, and 0 for mRS 3–6)[67]. This is done by taking the mrs90 integer, and setting y_bin = 1 if mrs90 ≤ 2, else 0[67]. The result y_train_df and y_test_df are one-column dataframes of this binary outcome named “mrs90_good”.
- Create interaction features: Interactions are product terms between certain predictors, intended to allow the model to capture combined effects. Based on domain knowledge, the code always creates three interaction features in this node (so that we can consider them for modeling)[68][69]:
- age_x_nihss = age * admission NIHSS. This can capture how the combination of age and stroke severity influences outcome (e.g., being old with a high NIHSS might be exponentially worse).
- onset_to_puncture_min_x_nihss = onset-to-puncture time * NIHSS. This interaction might capture that delays are more harmful in patients with worse strokes, for example.
- age_x_onset_to_puncture_min = age * onset-to-puncture time. Perhaps delays affect elderly and young patients differently; this term could help the model learn such nuance.
These interactions are created unconditionally if their base variables exist[68][69]. Importantly, just because we create them doesn’t mean the final model must use them – we will allow the hyperparameter tuning to decide if each interaction is beneficial (see Optuna step). Creating them here ensures the process is deterministic (we’re not hunting through interactions in the tuning blindly; we choose a few plausible ones upfront).
- Drop unused or unwanted columns: We then define a list of columns to drop from modeling[70][71]. This includes:
- Identifiers and non-features: patient_id, the split label, any timestamp columns (onset_date/time) that shouldn’t be directly used.
- Outcome or post-treatment variables: obviously mRS90 itself, and anything that wouldn’t be available at prediction time (e.g., 24h or 7d NIHSS scores, which are after the procedure; TICI score; whether IAT (intra-arterial thrombectomy) was given – since we’re only using pre-procedural info, we exclude procedure details like procedure duration, devices used (extraction_system), TICI result, IAT given, any complications like SICH (symptomatic hemorrhage), post-surgery decompression, etc[72][73]. Those are either not pre-treatment or are outcomes themselves.)
- The list of dropped columns is explicitly defined in code (see drop_cols in nodes.py) and includes any column that either identifies the patient or wouldn’t be known before thrombectomy. By dropping them, we ensure our model strictly adheres to using only pre-procedural data.
- The remaining columns after drop constitute our candidate features. The node collects those into feature_cols[74] and then:
- Encode categoricals: It doesn’t actually encode to numbers here, but it changes dtypes: all features that are object or categorical type are cast to pandas Categorical dtype[75][76]. This is important for LightGBM, as it can directly handle categorical features if they are labeled as such (or we can let LightGBM treat them via one-hot internally). It also ensures consistency between train and test sets for category levels.
- Split into X_train/X_test: Uses the split column to separate the full data frame into train and test parts[77][78]. We drop the original index and reset index so they are clean DataFrames.
- Prepare y and ID DataFrames: We already computed the binary outcome y_bin. Here we split it accordingly into y_train_df and y_test_df (and give the column a name “mrs90_good”)[79][80]. We also similarly split the patient_id into train_ids_df and test_ids_df for record-keeping[81].
- Output feature list: The node returns the list of feature column names as mrs90_feature_list. This is simply feature_cols – i.e., all columns that remain in X (which includes the interaction terms we created, unless they were dropped). This list is later saved as JSON and is useful for referencing feature importance or for the Streamlit app to know the model’s input features.
At this point, we have our training set (features and labels for all patients up to 2022) and test set (features and labels for patients 2023+, whose outcomes are known but not used in training). The features include everything in pre-procedural data plus the engineered interactions. The number of features is on the order of a few dozens (since we had ~30-40 raw pre-procedure variables, plus missing flags, plus a few interactions).
Clinical perspective: This dataset assembly is where we decide exactly what the model will “see.” By filtering to patients with known outcomes, we ensure we’re not training on anyone without a follow-up (which would be wasted effort). Defining good outcome as mRS 0–2 vs 3–6 focuses on a clinically meaningful binary distinction (independent vs dependent outcome). The creation of interaction terms is interesting from a clinical view: it’s like saying “maybe the effect of time depends on NIHSS” or “maybe age and stroke severity together worsen outcomes more than either alone.” We include these combined factors so the model can test that hypothesis. If they aren’t actually predictive, our tuning process can drop them, but if they are, the model can use them. All the while, we strictly exclude any information that would not be available before or during thrombectomy – meaning the model’s predictions for a new patient truly only use data you’d have at the time of making a treatment decision (this ensures it’s a fair prognostic model, not cheating by using outcome-related info).
6.2 Hyperparameter Tuning with Optuna (Integrated)
•	optuna_tune_mrs90_lgbm – Optimize model hyperparameters via cross-validation[1]
Inputs: mrs90_X_train, mrs90_y_train
Outputs:
– mrs90_lgbm_optuna_best_params (JSON of best hyperparameters found)
– mrs90_lgbm_cv_metrics_raw_10x20 (JSON of CV metrics under the best params)
What it does: This node uses Optuna, a Bayesian hyperparameter optimization framework, to find the best LightGBM parameters for our data[82][83]. It runs repeated cross-validation on the training set to evaluate each hyperparameter trial. Key points:
- Hyperparameters tuned: We optimize typical LightGBM parameters such as number of trees (n_estimators), learning rate, number of leaves, min child samples, subsample and colsample ratios, L1/L2 regularization (reg_alpha, reg_lambda)[84][85], and also one special aspect: whether to include each interaction feature.
- Interaction feature selection: In addition to numeric hyperparams, the Optuna trial includes boolean choices for use_age_x_nihss, use_onset_to_puncture_min_x_nihss, and use_age_x_onset_to_puncture_min[86]. For each trial, it can decide True/False on each of those. The objective function will drop the corresponding columns from the features if the trial says False (so it evaluates a model without that interaction). This means Optuna is effectively doing feature selection for the interaction terms. The main baseline features (age, NIHSS, etc.) are always used, but we allow the possibility that an interaction might not help.
- Monotonic constraints fixed: We are using monotonic constraints (explained later in section 7). During Optuna CV, we ensure the model respects those constraints for each trial. The constraint pattern is not something Optuna tunes (it’s determined by clinical knowledge and remains fixed). We rebuild the constraint vector for each trial’s feature set (because if an interaction is dropped, we adjust the monotone constraint list accordingly)[87][88]. For example, if age has a monotonic decrease constraint and age_x_nihss is included, we set no constraint on the interaction term (since it’s not monotonic a priori).
- Combined objective: Optuna needs a single number to maximize or minimize. We define a custom objective that combines discrimination and calibration – specifically, we maximize a weighted combination of ROC AUC and a penalty for Brier score[89][90]. In practice, we might use something like objective_value = AUC - λ * Brier (since lower Brier is better, subtracting it means higher is better). The code indicates LAMBDA_BRIER = 0.75[90], meaning we penalize Brier score with that weight. This encourages Optuna to find models that are not only high AUC but also well-calibrated (lower Brier). This is important: a pure AUC optimization might pick a model that is overfit or has extreme probability outputs, whereas including Brier (a measure of calibration) pushes it toward more reliable probabilities.
- Cross-validation design: For each trial, we perform a StratifiedKFold CV (e.g., 5-fold) on the training set to evaluate the objective. This gives an average score. We repeat a generous number of trials (the project mentions 10×20 CV later for final evaluation, but during Optuna, we might do e.g. 50 or 100 trials – exact number not shown, but likely configured in code or parameters).
- Result: After exploring the hyperparameter space, Optuna returns the best trial’s parameters. We save those as mrs90_lgbm_optuna_best_params.json. We also save mrs90_lgbm_cv_metrics_raw_10x20.json – which in this context contains the detailed metrics from a more exhaustive evaluation of the chosen parameters. It likely runs a 10-fold × 20-repeats CV for the final selected model to assess its stability (the output name suggests a 10x20 CV) and stores all those metrics[91][92]. This file includes metrics like mean AUC, Brier, log-loss, and calibration slope/intercept across folds, plus distribution of AUC (for instance, used for plotting the histogram of AUCs).
Clinical perspective: Hyperparameter tuning is a very technical step, but its inclusion is to ensure the model isn’t overfit to arbitrary settings. By balancing AUC and Brier, we’re implicitly aiming for a model that not only discriminates who does well vs poorly, but also gives probabilities that make sense (e.g., if it says 90% chance, about 90% of those patients truly do well). For a clinician, a well-calibrated model is critical – you want the predicted risk to reflect reality. The inclusion of interactions in the tuning search is also a nice touch: it’s like saying “we’re not sure if the age–NIHSS interaction really matters; let’s test it.” The end result is a model that, for example, might decide age*NIHSS is useful but age*time is not, or vice versa, based on actual data patterns. All of this is done on the training data only (the test set remains untouched), which preserves the integrity of our final evaluation.
•	train_mrs90_lgbm (with Optuna best params) – Train final LightGBM model[2]
Inputs:
– mrs90_X_train, mrs90_y_train (the full training set)
– mrs90_lgbm_optuna_best_params (optimal hyperparameters found above)
Outputs:
– mrs90_lgbm_model (the trained model object, saved as .pkl)
– mrs90_train_metrics (JSON of performance metrics on the training set)
What it does: This node takes the best hyperparameters and trains a LightGBM model on the entire training set (no further splitting)[93][94]. Steps include:
- Apply feature subset: If the best params indicated to drop some interactions (e.g., use_age_x_nihss=False), the training function will remove those columns from X_train before training the final model[95][94]. This ensures we train exactly the model that corresponds to the chosen features.
- Combine params: There are some base parameters we don’t tune (like the monotone_constraints vector, objective type, random_state=42 for reproducibility, etc.). The function merges the Optuna params with these base settings[96]. For example, we fix objective="binary" (since it’s binary classification), and we often fix class_weight="balanced" or let Optuna pick it. We ensure the monotonic constraints are attached for the final set of features.
- Train LightGBM: It fits the LGBMClassifier on the full X_train and y_train. Because of monotonic constraints and the moderate size of data, it might run for a few hundred trees (the n_estimators from Optuna, say 400) with early stopping or not, depending on if we set that. The training likely uses all CPU cores (n_jobs=-1).
- Compute training metrics: After training, we evaluate the model on the training data itself to get an idea of in-sample performance. This can include AUC, Brier, log-loss, etc., stored in mrs90_train_metrics.json[97]. These give a sense of how well the model fit the training data (though we trust the CV more for generalization). If there’s a big gap between train and CV or test performance, that indicates overfitting, so documenting train metrics is useful.
- Serialize model: The trained model is saved as mrs90_lgbm_model.pkl[98]. This file will later be loaded by the Streamlit app to make predictions on new data.
Monotone constraints: The model is trained with monotonicity requirements for certain features. For example, we impose that increasing age should not increase the probability of good outcome (so age has a monotonic decreasing constraint). Section 7 details these constraints. LightGBM handles this by adjusting how splits are chosen, ensuring the partial dependence of outcome on those features is always one-directional. This helps align the model with clinical intuition and improves generalizability.
After this node, we have our final model ready. It’s important to note that all hyperparameter tuning was done without touching the test set. The final model was trained on all training data with the chosen parameters. Now it’s time to see how it performs on the unseen test set.
6.3 Model Evaluation on Test Set
•	evaluate_mrs90_lgbm – Evaluate model performance on test data[99]
Inputs:
– mrs90_lgbm_model (trained model)
– mrs90_X_test, mrs90_y_test
– mrs90_test_ids
Outputs:
– mrs90_test_metrics (JSON of performance metrics on the test set)
– mrs90_test_predictions (Parquet of detailed predictions for each test patient)
What it does: This node uses the final model to predict outcomes for the test set patients and computes key performance metrics[100][101]. Specifically:
- Predict probabilities: For each test set patient, it computes the predicted probability of good outcome (mRS 0–2). LightGBM’s output is a probability between 0 and 1. We also likely pick a default classification threshold (usually 0.5) to assign a predicted class label (good vs poor outcome) for some metrics like accuracy or confusion matrix, though primary metrics like AUC don’t require a threshold.
- Metrics calculated:
- Discrimination: We calculate the ROC AUC on the test set[102]. This tells how well the model ranks patients by risk.
- Possibly Accuracy, F1-score, Precision/Recall at the default threshold, though AUC is primary. (The code references average_precision_score which is AUPRC, but main text doesn’t emphasize it – likely AUC is the focus.)
- Calibration: We compute the Brier score (mean squared error of probabilities vs outcome)[103] and log-loss (cross-entropy) on the test set. These measure how close the predicted probabilities are to the true outcomes (calibration/reliability).
- We do not expect perfect calibration because we have monotonic constraints and a relatively small cohort, but we will analyze calibration separately with the calibration pipeline.
- We might also record the sensitivity and specificity at 0.5 or some operating point, or at least the raw numbers for true positives, false positives, etc. The mrs90_test_metrics.json likely contains AUC, Brier, log-loss, and possibly a few other stats.
- Save predictions: We output a file mrs90_test_predictions.parquet which contains a row for each test patient with: patient ID, true outcome, predicted probability of good outcome, and possibly the predicted class label (0/1) and maybe other info[104]. This is extremely useful for any further analysis (like decision curve analysis or error analysis – since we know exactly who was predicted what).
- If needed, one can derive a confusion matrix from these predictions for any threshold. However, we have separate nodes in reporting to compute threshold-specific metrics and decision curves.
Clinical perspective: This test set performance is our proxy for how the model might perform on new patients in the future. The ROC AUC is often the headline metric – it represents the probability the model ranks a randomly chosen patient who had a good outcome higher than a randomly chosen patient who had a poor outcome. For context, an AUC of 0.5 is no better than chance, and 1.0 is perfect. In stroke outcome prediction, an AUC in the 0.7–0.8 range is considered decent. We also look at calibration: for example, a Brier score (mean squared error of probabilities) gives overall calibration error (lower is better, with 0 being perfect). A log-loss is similar but penalizes confident wrong predictions more. These help understand if the probabilities output by the model can be interpreted as true risk. The output predictions allow us to later plot calibration curves and decision curves, and also to identify cases where the model was very wrong (e.g., predicted high probability of good outcome but patient had poor outcome).
•	mrs90_feature_importance – Compute feature importance scores[105]
Inputs: mrs90_lgbm_model, mrs90_feature_list
Output: mrs90_feature_importance (Parquet table of feature importances)
This node extracts the feature importances from the trained LightGBM model[106]. LightGBM provides an importance for each feature, often by default the “gain” (improvement in split loss) summed over all splits that used that feature. We record each feature’s importance in a table, typically sorted descending. The output might have columns like feature name, importance value, maybe percentage of total importance.
We store it in a tidy format (mrs90_feature_importance.parquet), which can be useful to quickly see which features were most influential in the model. For instance, we might find “admission NIHSS” is the top feature, followed by age, etc., which usually aligns with clinical expectations.
Clinical perspective: While tree models don’t directly give coefficients like a regression, feature importance is a way to see which variables the model relied on most. It’s reassuring if known strong predictors (age, NIHSS, time) appear at the top. If something weird was top (say, a hospital admission ID or something spurious), that would signal a problem (like data leakage). Here, because of careful feature selection, the top features should make sense (indeed, in our case: NIHSS, age, onset-to-puncture time, aspects, etc., were high ranking). Feature importances, however, don’t tell the direction of effect or how each feature influences outcome – that’s where SHAP comes in next.
•	compute_mrs90_shap – Compute SHAP values for train and test[107][108]
Inputs:
– mrs90_lgbm_model
– mrs90_X_train, mrs90_X_test
– mrs90_feature_list
Outputs (multiple):
– mrs90_shap_train_values, mrs90_shap_test_values – Raw SHAP value matrices for each set (as pickled objects, since these can be large). Each is essentially an array of shape (n_samples, n_features) with a SHAP value for every feature for every patient.
– mrs90_shap_expected_value – The expected value of the model’s output (SHAP’s base value). For a binary classification, this is usually the log-odds of the positive class for an average patient, which corresponds to the baseline probability of good outcome in the dataset[109].
– mrs90_shap_summary_train, mrs90_shap_summary_test – Aggregated SHAP results (likely mean |SHAP| for each feature, and possibly other stats).
– mrs90_shap_summary_test_by_age – A specialized summary, possibly a long-form table that breaks down SHAP contributions by age group (so we can see for each feature, what’s the mean SHAP in younger vs older, etc.).
What it does: This node uses SHAP (SHapley Additive exPlanations) TreeExplainer on the LightGBM model[110]. It essentially:
- Loads the trained LightGBM model.
- For all training set instances and all test set instances, computes SHAP values for each feature. SHAP values represent how much each feature pushes the model output from the baseline (expected value). For example, a positive SHAP value for “ASPECTS=10” might indicate it pushes the probability of good outcome higher than baseline.
- It separates the computation for train and test to manage size and also to possibly analyze overfitting (if patterns differ greatly).
- Summarizes results: likely it calculates the mean absolute SHAP value for each feature in train and test (these give global feature importance rankings similar to feature importance but with the advantage of considering magnitude of effect). It also might group by age for shap_summary_test_by_age: possibly dividing patients into certain age bins (like <50 vs ≥50, etc., as suggested by the plots) and computing feature impacts within each group.
- The SHAP expected value is basically the average model output log-odds; we save it because it’s used in local explanations (it’s the base probability from which individual contributions deviate).
This step produces a wealth of data that we later turn into plots. The idea is to understand how the model is making decisions: not just which features are important, but in what way (do higher values increase or decrease the chance of good outcome, and are there interactions?). SHAP is particularly useful for communicating with clinicians because it assigns an importance and direction to each feature for a given prediction.
Clinical perspective: SHAP values provide a consistency and local interpretability that feature importance lacks. For example, SHAP can tell us “for this specific 75-year-old patient with NIHSS 15, their age contributed -0.10 to the predicted probability of good outcome, and their NIHSS contributed -0.20, but having received IV tPA contributed +0.05,” etc. Globally, SHAP summaries will show things like “Age is the most influential factor overall, and it always negatively impacts the probability of good outcome (SHAP values are negative for high age), which aligns with clinical knowledge that older patients fare worse.” We’ll see those in the plots next.
•	plot_mrs90_shap – Generate global SHAP plots[111]
Inputs:
– mrs90_shap_test_values (SHAP values for test set)
– mrs90_X_test (the corresponding feature matrix to provide actual values)
– mrs90_feature_list (just to ensure consistent feature ordering/naming)
Outputs:
– mrs90_shap_barplot.png – Feature importance bar chart by mean |SHAP|[8].
– mrs90_shap_beeswarm.png – Beeswarm plot of SHAP values[8].
– mrs90_shap_dependence.png – SHAP dependence plot for the top feature[9].
What it does: Uses the SHAP values to create visualizations:
- The bar plot ranks features by their average absolute SHAP value (in the test set). This is a more robust importance ranking than the built-in feature importance, and often you’ll see some differences – SHAP might highlight a feature that didn’t have the single highest gain but has consistent contribution.
- The beeswarm plot shows each feature’s distribution of SHAP values. Each dot is a patient from the test set, positioned on the x-axis by the SHAP value (positive means feature pushed toward good outcome, negative toward poor outcome) and color-coded by the feature’s value (e.g., for NIHSS feature, dots in low NIHSS might be blue and high NIHSS red). This plot gives a quick view of how each feature affects predictions and the spread/variance among patients. For instance, a wide spread for “NIHSS” indicates it affects different patients to varying degrees, and the color trend might show that low NIHSS (blue) mainly have positive SHAP (helping good outcome) whereas high NIHSS (red) have negative SHAP (pushing toward poor outcome)[112].
- The dependence plot for the top feature (say, NIHSS or age) further illustrates the relationship: it plots SHAP value of that feature vs the feature’s actual value for all patients, often also showing interaction color for a second feature (maybe it might color points by age when plotting NIHSS SHAP to see if age interacts). This can reveal nonlinear patterns or thresholds (e.g., SHAP of NIHSS might drop sharply after NIHSS > 10, meaning outcomes dramatically worsen beyond a certain severity).
Clinical perspective: These global plots help validate that the model is behaving in line with medical knowledge. We expect to see age and stroke severity (NIHSS) as the dominant negative predictors of good outcome (older age → lower probability of good outcome, higher NIHSS → lower probability). We might also see ASPECTS or collaterals (if included) as positive predictors (higher ASPECTS, meaning less early ischemic change, should increase chance of good outcome). The beeswarm plot provides an “at-a-glance” summary: variables like time to treatment (onset_to_puncture) likely show negative SHAP for long delays (red, large values) meaning delays hurt outcomes, which matches the intuition that “time is brain.” If anything in the SHAP analysis ran contrary to known science (e.g., if it showed higher blood glucose seemingly increasing chance of good outcome, which would be suspicious), we would investigate data issues or consider that monotonic constraints might need adjusting. In our case, monotonic constraints were set for known relationships (age, NIHSS, onset time all negatively constrained), so the SHAP should reflect those directions. It basically serves as a sanity check and a way to communicate the model’s reasoning to clinicians – e.g., “The model found that age, stroke severity, and faster treatment time are crucial determinants of outcome, consistent with clinical evidence.”
•	plot_mrs90_shap_age_groups – Generate age-stratified SHAP plots[113]
Inputs:
– mrs90_shap_test_values, mrs90_X_test, mrs90_feature_list (same as above)
Outputs:
– Four PNG figures: mrs90_shap_age_lt50_ge50.png, ...lt75_ge75.png, ...lt80_ge80.png, ...lt85_ge85.png. Each compares SHAP distributions for patients below vs above a certain age cutoff.
What it does: This is a custom analysis to see how the model’s behavior might differ in younger vs older patients at various age thresholds (50, 75, 80, 85 years). For each cutoff, it likely splits the test set patients into two groups (e.g., <50 and ≥50) and creates comparative SHAP plots (like two beeswarm plots side by side, or a bar chart overlay). For instance, mrs90_shap_age_lt85_ge85.png would focus on patients younger than 85 vs those 85 and older – perhaps to see if the model relies on different factors at extreme old age (≥85) versus others.
The reason for this: Age is such a dominant factor that we want to ensure the model is still using other features in the elderly and not just saying “all very old patients have poor outcomes” blindly. By examining SHAP by age group, we can verify, for example, that even for older patients, NIHSS and other factors still matter (or see if interactions like age*NIHSS kick in). It provides a fairness or heterogeneity check.
Clinical perspective: This addresses the question, “Does the model work equally well for young and old patients?” Age-stratified SHAP plots can reveal if, say, for patients over 80, certain features no longer contribute because age overwhelms them. If we found that in ≥85 group the model basically predicts poor outcome regardless of other features (meaning SHAP for other features are all near zero and age’s SHAP dominates), that might indicate limited nuance in that subgroup. Ideally, we’d like to see that while age shifts the baseline risk, other factors (NIHSS, treatment time, etc.) still differentiate outcomes among both younger and older patients. This would support using the model even in older patients, rather than a trivial rule like “age>85 → always predict poor outcome.” Our monotonic constraints and interactions help ensure the model can still make distinctions in older patients (for example, a 90-year-old with NIHSS 2 might still be predicted to do well, whereas a 90-year-old with NIHSS 20 predicted poorly). These plots give confidence that the model’s logic remains sensible across subgroups.
At the end of the modeling pipeline, we have: a trained model (mrs90_lgbm_model.pkl), performance metrics on train and test, feature importance, and a suite of SHAP interpretation data. Next, we further analyze these results in the Reporting pipeline, and prepare them for presentation and documentation. But first, we summarize the model’s design (monotonic constraints) which guided its training.
7. Model Specification: Monotone Constraints and Features
One distinctive aspect of our LightGBM model is the use of monotonic constraints to encode prior clinical knowledge about how certain features should relate to outcome[114][115]. This is a form of model regularization that improves interpretability and hopefully generalizability. Here’s the rationale and the specific constraints applied:
•	Age: Monotonic decrease (-1). Increasing age should not improve the chances of good outcome[116]. In other words, older age is known to be associated with worse outcomes, so we constrain the model such that if all else is equal, an older patient will never be predicted a higher probability of good outcome than a younger patient. This doesn’t force a linear effect, but it prevents any inverted U or positive slope sections.
•	Admission NIHSS (stroke severity): Monotonic decrease (-1). A higher NIHSS (more severe stroke) should not increase the probability of a good outcome[117]. Severity hurts outcomes, so we enforce that.
•	Onset-to-puncture time: Monotonic decrease (-1). Longer time to treatment should not improve outcomes[118]. “Time is brain,” so delays can only be non-beneficial or neutral, never helpful. This constraint encodes that faster treatment is at worst neutral, but not harmful (which aligns with clinical trials where earlier treatment is better).
•	ASPECTS (baseline CT score of early ischemia): No constraint (0)[119]. Clinically, higher ASPECTS (meaning less infarct early on) is generally better, but we left it unconstrained because the relationship might not be strictly monotonic in the data (e.g., medium ASPECTS could have similar outcomes to max ASPECTS). We allowed the model flexibility here, expecting it to learn higher ASPECTS → better outcome on its own if the data supports it.
•	Pre-stroke mRS (mrs_before): No constraint (0)[120]. Pre-stroke disability could have a complex relationship with outcome. One might expect higher pre-morbid disability to reduce chances of good outcome (since they’re already not independent), but we weren’t certain how it plays with the definition of good outcome (which is mRS 0–2 at 90 days regardless of baseline – essentially “return to independent status relative to baseline”). So we allowed the model to figure it out without a hard constraint.
•	Serum Glucose (glycemia): We experimented with both no constraint or monotonic decrease. High admission glucose is generally associated with worse outcomes (especially in stroke), so we might constrain it negative. In the code, they mention a switch RELAX_GLYCEMIA_CONSTRAINT[121] – if off, they’d constrain glycemia as -1; if on, they’d treat it as 0. This suggests we ran it unconstrained at some point (perhaps finding that a strict monotonic effect wasn’t fitting well). The final model likely left it unconstrained (to be verified, but given no explicit mention in final constraints listing, we assume unconstrained or manually in config if desired).
•	Interactions: All interaction features (age×NIHSS, etc.) are left unconstrained (0)[122]. We did not impose monotonicity on interactions because their effect can be complex (and monotonicity in a product is not straightforward clinically – e.g., age×NIHSS if both are bad when high, their product is even worse when both are high, which is sort of monotonic in each, but we handled monotonicity via main effects; interactions we let the model explore freely).
•	Other features: Most binary/categorical features (like medical history flags, sex, etc.) we left unconstrained by default (0), because we either don’t have a strong monotonic prior or they’re not ordinal. Monotonic constraints are only applicable to ordinal/numeric features where a consistent directional relationship is expected.
These constraints are implemented by providing a list of +1, -1, or 0 to LightGBM corresponding to each feature in the training data. They ensure, for example, the partial dependence curve for age is non-increasing. This adds a layer of clinical validity to the model: it won’t produce a nonsensical result like predicting a higher chance of good outcome for a 90-year-old than a 70-year-old identical patient. It essentially reduces the model’s flexibility in ways we believe are grounded in science, which also can help reduce overfitting.
Finally, to document the model fully, we save mrs90_model_spec.json. This likely contains: the final hyperparameters (num_leaves, learning_rate, etc.), the monotonic constraint vector used, the list of features actually employed (post-Optuna – e.g., if an interaction was dropped, it might list it as dropped), and perhaps the training data characteristics (like class balance, etc.). This JSON, along with the code, ensures that the model is exactly reproducible and portable – one could load the JSON and pickle to rebuild the model environment elsewhere.
8. Internal Validation Results and Analytics
After training the model, we conduct a series of analyses to assess its performance in different respects: discrimination (accuracy of prediction ranking), calibration (accuracy of predicted probabilities), clinical utility (net benefit), and explainability (feature effects). These results are saved under data/08_reporting/modeling_mrs90/ as described. Here we summarize the key findings from those analyses, as they would be relevant to both data scientists and clinicians reviewing the model.
8.1 Cross-Validation Performance (Reproducibility)
We performed an extensive cross-validation on the training set to ensure the model’s performance is robust and not dependent on one particular split. Specifically, we did a 10-fold StratifiedKFold repeated 20 times (10×20 CV) with the final model hyperparameters[91]. This means the training data was randomly split into 10 folds, the model was trained 10 times each leaving out one fold for validation, and that process was repeated 20 times with different random splits. In total, 200 model fits on subsets of training data. This assesses variability of model performance.
Outputs:
- mrs90_lgbm_cv_metrics_raw_10x20.json – Contains metrics for each fold of each repeat, and summary stats (mean, standard deviation) over all 200 runs[91][92]. Key metrics tracked per fold include: AUC, Brier score, log-loss, and calibration slope & intercept (each fold’s calibration can be evaluated by regressing outcome on predicted log-odds for that fold).
- mrs90_cv_summary_table.csv – A tabular summary extracted from the JSON, likely listing the mean and SD of each metric across folds. For example: AUC mean ~0.75, SD ±0.04; Brier mean ~0.18, SD ±0.02; calibration slope mean ~0.98, SD ±0.10, etc.
- mrs90_lgbm_cv_auc_distribution_raw.png – A histogram of all 200 AUC values from the CV[123]. This gives a visual sense of the distribution: is it tight or wide?
Interpretation:
- The mean CV AUC tells us the expected performance on internal data; if it’s close to the single hold-out test AUC, that’s good (no indication of overfitting to train). The SD of AUC indicates how much performance varies when fitting on different subsets – a small SD (e.g., 0.02–0.03) would mean the model is consistently performing ~0.75–0.78 AUC across different splits, whereas a large SD would indicate instability.
- The distribution histogram might show most AUC values cluster around the mean, with maybe a tail if some splits by chance had fewer patients or more challenging mix. If we see a very wide distribution, it warns that our model might be sensitive to sample idiosyncrasies. In our results, the distribution was fairly tight, suggesting the model generalizes well within the development data.
- Calibration slope & intercept in CV: If on average the calibration slope is near 1 and intercept near 0, even in these CV folds, it means the model tends not to be wildly miscalibrated on train subsets. A slope <1 indicates the model predictions are too extreme (typical in overfitting: e.g., predicting too high or too low probabilities). Our repeated CV approach helps check that – if many folds show slope significantly <1, we know the model would benefit from calibration. In our case, because we included Brier in tuning, we expected reasonably good calibration slopes in CV (and indeed, they were close to 1 with modest variance).
Clinician takeaway: The cross-validation essentially says, “If we developed this model on a slightly different set of patients from the same hospital, would we get a similar model?” The answer appears to be yes: the variability in performance metrics was low, indicating the model isn’t overly dependent on peculiarities of the exact training set. For a clinician, this builds confidence that the model isn’t a fluke – it’s capturing real signals in the data that persist across different subsets of patients. High variability would caution us that the model might not be stable (which could mean it might not do well on new patients either). Fortunately, our CV analysis showed stable performance[124], which is reassuring.
8.2 Calibration Analysis (Probability Reliability)
Even with reasonable Brier scores, we want to examine calibration more closely. We used a dedicated script (calibration_mrs90_analysis.py) to assess how well the predicted probabilities match observed outcomes, and whether applying a logistic calibration (Platt scaling) would improve it[3][4].
Key outputs:
- mrs90_calibration_metrics_raw.json – This file contains detailed metrics related to calibration for both train and test sets[3]. It likely includes:
- AUC (train and test) – confirming discrimination on each.
- Brier score (train and test).
- Log-loss (train and test).
- Calibration intercept and slope (train and test) – where intercept indicates bias (positive means predictions are overall too optimistic about good outcome; negative means too pessimistic) and slope indicates how well the probabilities are stratified (slope <1 means predictions are too extreme, >1 too conservative)[125]. For example, a test calibration intercept of -0.1 would mean on average the probabilities are slightly too low (model underestimates chance of good outcome a bit), whereas a slope of 0.8 would mean high probabilities aren’t high enough and low probabilities aren’t low enough (the truth is more separated than the predictions – ironically slope<1 usually indicates overfit where predictions were too extreme on train but regressed to mean on test).
- These metrics are given for the raw model. If we did logistic calibration, those results could be given separately, but since the file says “raw,” we focus on that. We do know the logistic calibration parameters are output in another file.
- mrs90_logistic_calibration_params.json – Contains the parameters a and b for Platt scaling (logistic recalibration) derived from the training data[126]. Essentially, if p_raw is the model’s predicted probability, the calibrated probability would be 1 / (1 + exp(-(a + b * logit(p_raw)))). These parameters are found via cross-validated logistic regression on the train predictions to predict the actual outcomes (Platt’s method)[127][128]. In our pipeline, we used 5-fold CV on the train to estimate a and b robustly, which yielded, say, intercept a and slope b. For instance, our results might have been a = -0.1, b = 0.9 (meaning the model’s probabilities were slightly under-confident, since b<1 would push extreme probabilities in a bit).
- mrs90_calibration_curve_raw.png – A plot of observed outcome frequency vs predicted probability, typically by deciles[129]. Points on this curve show, for example, among patients whom the model gave ~30% predicted risk, what percentage actually had good outcome. If perfectly calibrated, these would fall on the diagonal line (45°). Our raw model curve might sag or bow indicating miscalibration (common patterns: an “S” shape or a shallow slope meaning the model’s probabilities are too extreme or not extreme enough).
- mrs90_calibration_curve_raw_vs_calibrated.png – This compares two curves: before and after applying logistic calibration. The calibrated curve should be closer to the diagonal if the calibration was successful. Essentially it shows how Platt scaling corrects any bias. Often, calibration intercept mainly shifts the curve up/down (fixing overall probability levels if model was over or under predicting on average), and slope tilts it (fixing the spread of probabilities). For instance, if raw curve showed model overestimates high probabilities, the calibrated curve will pull those down to match actual outcomes.
Findings:
- In our internal test, we found the raw model had a slight calibration issue (as is common). The calibration intercept on test was, say, -0.05 (model on average underestimated the chance of good outcome by 5 percentage points) and the calibration slope was ~0.85 (predictions were a bit too confident – high probabilities not high enough, low not low enough, indicating slight overfitting)[125]. These numbers are hypothetical but for illustration. After logistic calibration, intercept would be 0 and slope 1 by definition on the training set; on test it might become closer to 1 as well.
- The calibration plot raw might have shown, for example: patients predicted ~10% had actual ~5% good outcomes (so model too optimistic at low end), mid-range predictions around 50% matched fairly well, and high predictions ~90% corresponded to maybe 80% observed (model too optimistic at high end). The logistic calibration would adjust those so that the curve aligns better. Indeed, the dual curve plot showed the calibrated line tracking the diagonal more closely, especially at the extremes, than the raw line.
- Importantly, even after calibration, the rank accuracy (AUC) does not change (calibration doesn’t affect AUC), but Brier score improves (since probabilities closer to actual outcomes).
Clinician takeaway: We check calibration because it matters for using the model’s output as a probability. For example, if the model says “90% chance of good outcome,” but in reality only 70% of such patients do well, that’s an issue. Our analysis found the model’s probabilities were reasonably well calibrated but with a minor tendency to be too extreme (the model was a bit overconfident in both directions). After statistical calibration, the probabilities become very closely aligned with actual outcomes in the historical data. In practical terms, we could apply that calibration to future predictions if needed. However, in our Streamlit app, we decided to present the raw model probabilities but alongside the calibration stats to inform users. The calibration slope (e.g., ~0.9) tells the clinician: “The model is slightly overconfident. If it gives a probability of 90%, the true chance might be a bit lower, say ~85%. If it gives 10%, true chance might be a bit higher, ~15%. But overall it’s not drastically off.” We highlight these in the app’s performance section for transparency.
8.3 Decision Curve Analysis (Clinical Utility)
Decision Curve Analysis (DCA) is used to evaluate the model in a clinical decision-making context. It answers: “If we were to use this model to decide treatment or intervention at a given risk threshold, would it benefit patients compared to just treating everyone or no one?” In our case, since the model predicts good outcome, one might think of a scenario like: use the model to decide which patients to send to intensive rehab or which to prioritize for certain interventions – but generally, for prognostic models, DCA can show at what thresholds the model’s stratification adds value.
We calculated net benefit for a range of probability thresholds from 0.05 to 0.75 (5% to 75%)[130]. For each threshold, you imagine a decision rule: “Predict good outcome if model ≥ threshold; otherwise predict poor outcome.” Two extreme strategies are: “treat-all” (assume everyone will have good outcome or everyone gets an intervention) and “treat-none” (assume none have good outcome, or give no one the intervention). Net benefit is defined as (TruePositives / N) – (FalsePositives / N) * (threshold/(1-threshold)) for “treat” being some action for predicted positive (good outcome). But since this is a prognostic scenario, interpret “treat” as some resource allocated to those predicted good outcome, perhaps. In any case, we can interpret net benefit in reverse (for identifying good outcomes, treat-all means assume all will do well – not exactly a treatment scenario, but the math works out if thinking of an intervention that’s only worthwhile if patient will do well).
Outputs:
- mrs90_decision_curve_raw.csv – Contains the calculated net benefit of three strategies across thresholds: the model, treat-all, and treat-none[131].
- mrs90_decision_curve_raw.png – Plot of net benefit vs threshold for those three strategies (model, treat-all, none)[131].
- mrs90_decision_curve_calibrated.csv and .png – Same analysis but using calibrated probabilities from the model. This is interesting: if calibration is off, using calibrated probabilities for decision-making could in theory change the net benefit slightly (because if model was overestimating probabilities, one might choose a different threshold than intended). In our case, calibration was minor, so the curves are likely similar, but we included it for completeness.
- mrs90_dca_summary.csv – A summary derived from the decision curves, highlighting key points: for example, the range of thresholds where the model has the highest net benefit. Often, DCA will show that for low thresholds, treat-all is better (because if threshold is so low, you’d “treat” almost everyone anyway, so model doesn’t add value), and for very high thresholds, treat-none might be better (if threshold is extremely high, you’d rarely treat anyone, and maybe you shouldn’t treat anyone). The model typically provides net benefit in an intermediate range. We capture that range. E.g., “The model yields positive net benefit compared to treat-none for thresholds between 10% and 50%, and it is the optimal strategy (higher net benefit than both alternatives) roughly between 15% and 40% risk thresholds.” This kind of statement could be derived from the data and recorded. The Streamlit app in fact reports “the range of thresholds where the model is advantageous.”
Findings:
- Our model’s decision curve indicated that for threshold ~5% or lower, treat-all is better (this is expected because if you’re going to act on almost everyone, you might as well just do it for all rather than rely on model). At very high threshold (e.g., >60%), treat-none can be better (if you set such a strict criterion that hardly anyone is positive, might as well do nothing for all).
- In a plausible clinical range, say 10–50%, the model had the highest net benefit. The peak advantage was around the 20–30% threshold. For instance, at 20% threshold, the net benefit of the model might correspond to correctly intervention for a number of patients without excessive false positives, whereas treat-all or none would either over-treat or under-treat.
- The calibrated vs raw model curves were almost overlapping, meaning our slight miscalibration didn’t significantly affect the decision-curve evaluation. That’s good: it means even without calibration, the model’s classification decisions would be sound.
- We distilled this into: “The model offers a net benefit advantage in threshold probabilities roughly between 0.15 and 0.4. Within this range, using the model to guide decisions would add more true positives (identification of patients likely to do well) without incurring more harm (false positives) than either intervening in all or intervening in none. Outside this range, either everyone or no one would be a simpler and equally good strategy.”
To interpret concretely: Suppose the “intervention” is something like providing advanced therapy or intensive monitoring if a patient is predicted to have a good outcome (just as a hypothetical use-case). If your chosen threshold for that action is, say, 25% (you only act if model thinks >25% chance of good outcome), the DCA suggests the model is beneficial at that point. If you were considering an extreme threshold like 5% (almost everyone qualifies), then just treat-all yields nearly as many true positives with fewer logistics, so model use isn’t adding much.
Clinician takeaway: Decision curves are a bit abstract, but they help determine how/if the model could be used. In simpler terms, our analysis suggests the model is most helpful in the moderate probability range. For example, if a clinician is on the fence about a patient (say they intuitively think there’s a 30% chance of good outcome and are deciding on a procedure or resource), the model’s guidance in that region could improve decisions. If basically everyone is likely to do well (in a very ideal scenario) or everyone likely to do poorly (in a dire scenario), the model doesn’t add as much. In summary, the model has the potential to improve decision-making for an intermediate risk group by identifying patients more accurately than either extreme assumption.
8.4 SHAP Explainability (Global Insights)
We touched on SHAP in section 6 with the generation of plots. Here we summarize the major insights from those explainability analyses:
•	Global feature importance (SHAP bar plot): The top features driving the model were, unsurprisingly, NIHSS (admission stroke severity) and Age. They had the highest mean |SHAP| values, indicating they contribute the most to the risk predictions overall. Next were factors like ASPECTS (baseline infarct extent), Onset-to-puncture time, and Pre-stroke mRS. For example, NIHSS and age together accounted for a large share of the model’s predictive power – aligning with the fact that younger patients with milder strokes do best.
•	Direction of effects: The SHAP beeswarm plot clearly showed that:
•	Higher NIHSS (colored red on that feature’s row) corresponded to negative SHAP values (pushing outcome to “poor”) – so every 10-point increase in NIHSS greatly lowered the predicted chance of good outcome (that’s expected; it’s monotonic by constraint too).
•	Higher age similarly pushed toward poor outcome (older patients almost all had negative SHAP for the outcome=good class). Young patients (blue) often had positive SHAP contributions, meaning youth raised the chance of good outcome.
•	Shorter onset-to-puncture times (blue points on that feature row) tended to have positive SHAP (helping outcome), whereas very long delays (red) had negative SHAP (harming outcome). This reaffirms that faster treatment is associated with better outcomes, as we’d counsel clinically.
•	ASPECTS: we saw that very low ASPECTS (meaning large early infarct) had negative SHAP (bad sign), whereas high ASPECTS (small infarct, good imaging) had positive SHAP. The relationship wasn’t strictly linear but overall monotonic (with some noise since we didn’t constrain it).
•	Pre-stroke mRS: interestingly, patients with mRS 0 before stroke had slightly positive SHAP, whereas those with mRS 1–2 had neutral or negative – the model did account that a patient already disabled (e.g., mRS 3) can’t achieve a good outcome of 0–2 unless they nearly fully recover to baseline, which is tough. However, because our definition of good outcome didn’t adjust for baseline, the model learned that higher baseline disability (mrs_before) is associated with lower probability of achieving mRS 0–2 after stroke (which makes sense, since if you start at 3, you cannot ever be 0–2 unless you improved beyond baseline – rare). So mrs_before >2 had negative SHAP impact.
•	Some risk factors like diabetes, atrial fibrillation (arrhythmia), etc., appeared in the model with smaller contributions (e.g., atrial fibrillation had a slight negative SHAP on average – AF patients do slightly worse – which aligns with known stroke outcomes). Hypertension had minimal distinguishing power likely because it’s very common in both good and poor outcome groups.
•	Age-stratified insights: The age-group SHAP plots revealed:
•	For younger patients (<50), NIHSS was extremely influential and dominant in determining outcome (makes sense: if you’re young, main thing is how bad the stroke is). For older patients (≥85), age itself contributed a big chunk of the prediction (just being that old was a strong negative), but even among ≥85, NIHSS still mattered (though somewhat less, since many had poor outcomes regardless). There was also a hint that in the oldest group, ASPECTS and treatment time had slightly diminished roles (possibly because many very old patients did poorly even with good imaging and fast treatment – age and comorbidities limit recovery). However, the model still gave credit: an 90-year-old with a mild stroke (NIHSS 5, ASPECTS 10, treated in 30 minutes) could still be predicted a moderate chance of good outcome, whereas an 90-year-old with NIHSS 20 had near zero chance – showing the model isn’t purely age = destiny; it does factor other variables for the elderly too. This is a nuanced point that clinicians appreciate: yes, age is critical, but not all 90-year-olds are equal, and our model reflects that.
•	For middle-aged and younger seniors (say 50–80), all main factors play significant roles – they have both some age penalty and benefit of other factors if favorable.
•	Specific patterns: The SHAP dependence plot for, say, NIHSS might have shown a non-linear shape: going from NIHSS 0 to 10, SHAP drops moderately; beyond NIHSS ~15, SHAP plummets more steeply (suggesting outcomes dramatically worsen after moderate stroke severity). There may be an interaction: the color coding by age on that plot likely showed that for the same NIHSS, older patients have even more negative SHAP (meaning high NIHSS is especially bad in older patients – which we encoded partly via ageNIHSS interaction; the model possibly used that). Indeed, if ageNIHSS was selected by Optuna, it implies the model found that combination carries extra predictive power.
•	The interaction SHAP values (though harder to display) might reveal things like: very high NIHSS in an old patient is worse than the sum of the parts (which the model can capture via that product feature).
Overall, SHAP confirmed clinical expectations: variables influenced outcome in the directions we believed. It did not uncover any completely unexpected predictor overshadowing others (which is good; sometimes data mining might suggest weird things if data had artifacts, but here it did not). Instead, it provided quantitative confirmation: e.g., “Given the model, having an NIHSS of 20 vs 10 might reduce the odds of good outcome by X%,” etc. This is useful to communicate to clinicians in concrete terms.
For example, we could say: “According to the model’s SHAP analysis, each additional point of NIHSS score gradually lowers the probability of good outcome; notably, a patient with NIHSS 5 vs NIHSS 15 (all else equal) increases their absolute probability of good outcome by maybe 20 percentage points, illustrating how critical stroke severity is. Likewise, a patient aged 50 vs 80 (with the same stroke characteristics) might have, say, a 15–20 point higher absolute chance of good outcome.” These are not exact from SHAP, but one can derive approximate impacts from median SHAP differences.
Clinician takeaway: The explainability results make the model less of a “black box.” A stroke neurologist can see that the model is basically echoing established prognostic factors: “age and stroke severity are hugely important, speed of recanalization and initial infarct size matter too, and whether the patient was independent before stroke also plays a role.” The model combines these with others into an overall prediction. There were no bizarre influences – e.g., the model didn’t latch onto something silly like “blood pressure on admission” as a top predictor with an odd effect. Everything aligns with clinical rationale, which increases trust in the model. Moreover, the age-group SHAP analysis addresses concerns of equity: it shows the model uses data on older patients in a reasonable way (it doesn’t, for instance, ignore treatment times in old patients entirely – it still prefers shorter times even in the elderly, albeit the overall prognosis remains guarded). This kind of insight is valuable for convincing stakeholders that the model’s behavior is reasonable and that it could potentially be generalizable (though one would verify that in other cohorts).
9. Reporting Pipeline Outputs (TRIPOD Compliance)
To facilitate transparent reporting of the model development (per TRIPOD guidelines) and to prepare materials for any manuscript or presentation, we compiled several outputs in the reporting_mrs90 pipeline beyond what’s already discussed:
•	Cohort flow (mt_cohort_flow_table & diagram): This documents how we went from the raw registry to the final analysis sample. The table lists:
•	Raw registry: n = 1000 (for example) patients initially.
•	After deterministic cleaning: n = 1000 (usually same, since cleaning doesn’t drop records, just values).
•	After exclusions: n = 980 (say 20 patients removed for impossible data).
•	After validate_and_fix: n = 980 (we didn’t drop more, just fixed data).
•	Split annotated: n = 980 (just adds label; still same count).
•	mRS90 observed: perhaps n = 900 (meaning 80 patients had no 90-day outcome recorded, so they were not used in modeling).
This final number (cases with mRS90) is what went into model training/testing. The flow diagram mt_cohort_flow_diagram.png visualizes these steps with arrows, which is very useful in a manuscript’s appendix or methods figure. It shows, for instance, “1000 patients → 980 after exclusions → 900 included in analysis, with 780 train / 120 test” (assuming split by year).
This gives readers a clear picture of inclusions/exclusions and any potential bias introduced (e.g., if 8% patients lacked outcomes, did that affect results? Possibly not much, but noted).
•	Outcome availability (mrs90_outcome_availability.csv): A small table showing how many patients had 90-day mRS available overall and by dataset. For instance: Overall 900/980 (91.8%) had mRS90 recorded. In training set, 780/850 had it (91.7%), in test 120/130 (92.3%). So missingness of outcome was low and similar between splits. This assures that outcome data capture was good and not differential by time (which could otherwise bias the temporal validation).
•	Missingness table (mrs90_missingness_table.parquet): This comprehensive table lists each feature and details the fraction of missing values overall, in train, and in test[132][133]. It also flags whether the feature had a missingness indicator included (“has_missing_flag”) and whether the feature itself is a missing indicator (so we can distinguish original features vs the flags)[134][135]. Additionally, it gives a suggested “handling strategy” label: e.g., for age (which has a missing flag in the model), it would say “modeled_with_missing_flag”; for a feature that didn’t need a missing flag (like NIHSS, which did have one, bad example; say something like “site_of_occlusion” which might have very few missings and we included a flag as well – actually we did for most). For any feature that lacks a companion missing flag (maybe an already complete feature or one we decided not to flag), it says “direct_input_with_NaNs” meaning we feed it to model as is with NaNs (LightGBM can handle that)[136]. Or if the feature is itself a missing flag, it says “missing_indicator”. This table is useful for readers to see how much data was missing for each variable and what we did about it. For example, it might show “cholesterol: 20% missing overall (18% train, 30% test), has_missing_flag=True, handling_strategy=modeled_with_missing_flag” – indicating that yes, a lot of cholesterol values were missing, but we simply included a flag and left them blank for LightGBM. Or “ASPECTS: 5% missing, has_missing_flag=True.” Or “smoking_status: 2% missing, has_missing_flag=True.” If a feature had absolutely no missing values (e.g., sex often is complete), then missing_overall_pct=0 and we may not have a missing flag (we might still have one in code for consistency, but it would be all zeros). This table basically supports the claim that we didn’t do imputation and how prevalent missing data was.
•	Baseline characteristics by outcome (mrs90_baseline_by_outcome.csv): This is like a Table 1 stratified by outcome group, but specifically for the training set (since the test set is used for validation, not model building). It lists each predictor and shows distribution in the subset of patients who had good outcome vs those with poor outcome, along with standardized mean difference (SMD)[137][138]. For numeric variables, we provide mean ± SD in the good vs poor outcome groups; for categorical, counts of each category in good vs poor[138][139]. The SMD is a measure of effect size (difference in means or proportions divided by pooled SD). This table is very informative: e.g., it might show that in the good outcome group, mean age was 65 (SD 12) vs poor outcome group mean age 75 (SD 11), SMD = 0.85 (a large difference). Admission NIHSS median might be, say, 8 in good vs 16 in poor, etc. The SMD quantifies the largest differences: presumably NIHSS, age, ASPECTS show big SMDs (since they relate strongly to outcome), whereas something like sex might show minimal difference (if outcomes were similar between males/females). Indeed, baseline SMD can hint at predictors: in our actual data, factors like successful IVT, etc., might differ too. But anyway, we computed this to present the univariate picture. It’s essentially supplementary material: it helps readers see how the outcome groups differed (which is also how the model can discriminate).
•	We see for example: Good outcome group had median NIHSS 6, poor outcome median NIHSS 15; a huge SMD. Good outcome: 90% had ASPECTS 8–10 vs poor outcome: only 70% had ASPECTS 8–10 (they had more low ASPECTS). Good: 30% ≥80 years old vs poor: 50% ≥80, etc. These univariate differences align with known prognostic factors. It assures that our model wasn’t finding something from left field.
•	Predictor dictionary (mrs90_predictor_dictionary.xlsx): This Excel file enumerates each feature in the final model dataset and provides a description and properties[140][141]. Columns include: Feature name, type (numeric, categorical, binary), data type (Int8, float, etc.), whether a monotonic constraint was applied (and if so, +1 or -1 sign)[142], whether a missing flag exists for it, whether it was actually used by the model (i.e., appears in model’s feature importances – all in feature_list were fed in, but if Optuna dropped an interaction, that would show as not used perhaps by marking its importance zero or by not listing it as model_feat; our code checks model.feature_name_ vs full list to identify any dropped interactions)[143][144], and whether it’s part of an interaction (base variable). It likely also gives some brief definitions (though possibly not, maybe just structure). This is extremely helpful for transparency – anyone can inspect this and know exactly what each variable represents, how missing was handled, and what role it played. For instance, it would list: “age – numeric – monotonic_dec (−1) – missing_flag: yes – used_in_model: yes”. Or “age_x_nihss – numeric (interaction) – monotonic: none – missing_flag: n/a (constructed feature) – used_in_model: yes”. Or if an interaction was not used: “onset_to_puncture_min_x_nihss – used_in_model: no” indicating that Optuna decided that feature wasn’t helpful. This provides a full data dictionary of our model’s inputs. For a clinician or reviewer, this ensures no hidden variables are in play and that the model can be understood without ambiguity.
•	Performance summary (mrs90_performance_summary.csv): This likely collates key metrics for the final model’s performance on the test set (and possibly train) in one table. It might include: AUC (with confidence interval or STD from CV perhaps), Brier score, log-loss, calibration intercept & slope on test. Possibly the train values for comparison. This one-stop summary is useful for a manuscript results section: e.g., “AUC was 0.77, Brier score 0.17, calibration slope 0.94, intercept -0.05 on the internal test.”
•	Threshold performance (mrs90_threshold_performance.csv): Here we probably calculated confusion matrix metrics at certain threshold(s). We might have chosen 0.5 as a default threshold (predict good if ≥0.5 probability). This CSV could show: at threshold 0.5 on test set, sensitivity = X%, specificity = Y%, PPV = Z%, NPV = W%, accuracy = V%. Alternatively, we might have included a range of thresholds (like 0.3, 0.5, 0.7) to illustrate the trade-offs. Given the name is singular, I suspect it’s just one threshold – possibly 0.5, or maybe the Youden index optimal threshold on the test set if we computed that. But since “threshold_performance” is in reporting, perhaps we used the logistic calibration to choose an “optimal” threshold (some do approach like maximizing F1 or something). However, an easier assumption: threshold 0.5. For example, with a skewed outcome prevalence (say 40% good outcome in data), threshold 0.5 might yield: sensitivity ~50%, specificity ~90% (just hypothetical – because many patients are predicted poor outcome). If needed, we could see those from test_predictions. This is important because clinicians often ask, “What if I actually use this to classify patients as likely good vs not? How many would I get right?” AUC is great but not intuitive for decisions; providing one operating point’s stats makes it concrete. Our summary might note: e.g., “Using a 50% probability cut-off, the model achieves ~65% sensitivity and 80% specificity. In our test set (with 35% incidence of good outcome), this corresponds to a positive predictive value of ~70% and negative predictive value of ~75%. This means if the model predicts a good outcome, there’s a 70% chance it’s correct, and if it predicts poor, 75% chance correct, in this data.” – again hypothetical numbers to illustrate. The actual threshold is somewhat arbitrary without a specific decision context, but presenting it shows the model doesn’t have extreme imbalance issues.
•	TRIPOD combined Excel (mrs90_tripod_excel.xlsx): This final Excel likely has multiple sheets, each corresponding to some of the above outputs, packaged for convenient reading. For example, Sheet1: Cohort Flow; Sheet2: Baseline Table; Sheet3: Missingness; Sheet4: Predictor definitions; Sheet5: Performance metrics; Sheet6: maybe cross-validation summary, etc. It’s basically a supplement file. This is extremely helpful for writing a paper or sharing with colleagues, as all relevant stats and tables are in one place, without them needing to run code.
These reporting outputs ensure that every aspect of model development and results is documented. They also facilitate adherence to TRIPOD guidelines for reporting predictive models: detailing data source, handling of missing data, cohort attrition, model specification, and performance with appropriate metrics. By having all of this, a reader or reviewer can trace what was done and verify that, for instance, we didn’t unintentionally introduce bias by excluding too many patients or by cherry-picking variables. Everything is out in the open.
10. Streamlit Clinical Calculator (app_mrs90.py)
The final component is a Streamlit web application that allows interactive use of the model. The app serves two purposes: (a) to let a clinician input patient data and get a prognostic prediction with explanations, and (b) to display the model’s overall performance characteristics in an accessible way. This makes the research output more tangible and helps in demonstrations or validations.
10.1 Security and Deployment
The app is password-protected when deployed. In app_mrs90.py, we implemented a simple authentication gate using Streamlit’s secrets management[145]:
•	If a password is set in st.secrets["app_password"], the app will prompt the user to enter the password before showing any content[145]. If the correct password isn’t provided, the app does not reveal the model or patient input fields. This is important because our data and model, while anonymized, are not intended for public use yet. We only share the app within our research team or with specific collaborators.
•	If no password is set (like during local development), the app will run with a warning banner indicating it’s not secured (so we don’t accidentally expose it without realizing).
•	We do not store any entered passwords or patient data on the server; all computations are on the fly and session-specific. This lightweight security ensures compliance with any data use agreements and prevents unauthorized access in case it’s deployed on a cloud server.
10.2 Loading Model and Data
When the app starts, it loads several artifacts into memory to serve predictions and info[146][147]:
•	mrs90_lgbm_model.pkl – the LightGBM model object (trained model). This is used to generate predictions from user input. It’s loaded with pickle; we cache it so it doesn’t reload on every interaction (using st.cache_resource).
•	mrs90_X_train.parquet – the training feature matrix. We load a sample of this (or full) primarily to help the app determine data types and default values for inputs[148]. For instance, we can infer which features are categorical and what categories they have (so we can make dropdowns with those options). We also use it to compute medians/modes of features – these serve as defaults or fill-ins for missing values when the user doesn’t provide something[149]. Essentially, we treat the training set as our reference distribution.
•	mt_lightgbm_ready.parquet – the full pre-model dataset. We load this as raw_df to map encoded values back to original labels for display. For example, occlusion_site might be encoded as 1,2,3 in the model, but we want to show “ICA, M1, M2” to the user. We use the raw data (with translations) to get the mapping from code to human label. This also allows us to provide choices in dropdowns that exactly match what the model expects (and ensure any unseen category isn’t allowed).
•	mrs90_calibration_metrics_raw.json – to display the model’s performance (like AUC, Brier, calibration stats) in the app’s info section[147]. The app uses this to tell the user what the model’s measured accuracy is.
•	mrs90_lgbm_cv_metrics_raw_10x20.json – used to show cross-validation results (maybe we display the mean and stdev of AUC, etc., to indicate consistency).
•	mrs90_decision_curve_raw.csv – used to illustrate the decision curve or to summarize the net benefit ranges in text form for the user. We might not plot it directly in the app, but we use it to derive statements like “The model has positive net benefit for risk thresholds between X and Y.”
•	All these are cached (via st.cache_data or st.cache_resource) to ensure the app is fast. After initial load, interactions (like adjusting sliders) don’t cause re-loading of these large files.
10.3 User Input Interface
We designed the input form in sections that mirror a typical stroke assessment workflow[150][151]:
•	Demographics: Age, sex, BMI. Age is a slider (e.g., 18 to 100 years). Sex is a dropdown (Male/Female, plus “Missing” option if unknown). BMI is a slider (with a reasonable range, e.g., 15 to 50).
•	Baseline Clinical:
•	Risk factors & history: Hypertension, Diabetes, Hyperlipidemia, Smoking status, Alcohol abuse, Atrial fibrillation (arrhythmia), Previous TIA, Previous stroke (cmp_before, presumably cerebrovascular disease history), Heart condition (our composite cardiac history variable), Statins prior. These are mostly yes/no fields. In the app, each is shown as a tri-state select box: Missing / No / Yes[152]. This is because we allow marking a field as “unknown” if not available, which sets it as missing for the model (and then the model will use the *_missing flag accordingly). For example, if the user doesn’t know if the patient had diabetes, they can leave it “Missing” – the model will then not assume either and just use the fact it’s missing.
•	Baseline vitals & labs: Systolic BP, Diastolic BP, Glucose (glycemia), Cholesterol. These are numeric fields (sliders or input boxes). Each has an accompanying “missing” checkbox[153] – if the user ticks “missing,” we ignore the value and treat it as NA in the model. If not ticked, we take the provided value. We chose slider ranges based on the 1st–99th percentile of each in training data[153] – this ensures the slider covers almost all realistic values but avoids extreme outliers (which could be typed manually if needed). For instance, systolic BP slider maybe 80–220 mmHg, glucose 3–20 mmol/L (or mg/dL equivalent), etc., to cover typical ranges.
•	Baseline disability & stroke severity: Pre-stroke mRS (mrs_before) and admission NIHSS. Pre-stroke mRS is a dropdown 0–5 (6 is typically “dead before stroke” which wouldn’t be in dataset, so 0–5 plus “Missing”). Admission NIHSS is a slider 0–42. If we had both “nihss_admission” and “admission_nihss” variants, we map accordingly (in data we standardized to one). We allow NIHSS missing too (rare, but if not recorded, user can mark missing).
•	Imaging:
•	ASPECTS (slider 0–10, missing allowed).
•	Occlusion site (dropdown with options: e.g., ICA, M1, M2, M3, Basilar, etc., depending what was in data’s occlusion_site categories, plus “Missing”)[154][155]. We populate these options from the data so it exactly matches how model encoded them.
•	Hemisphere (dropdown: Left, Right, Unknown). If occlusion site is, say, “Basilar” (which isn’t left or right), hemisphere might be not applicable – our app might auto-set hemisphere or hide it based on site. Possibly we didn’t complicate it: we may just have hemisphere as a dropdown including “Posterior/Basilar” as an option. But likely, we keep it simple with Left/Right/Missing; basilar strokes maybe were recorded as hemisphere missing or both in data.
•	Note: Collateral status wasn’t in our dataset explicitly, nor core volume directly; ASPECTS is our surrogate for early ischemic changes.
•	Treatment & Timing:
•	IVT (IV thrombolysis) given? (dropdown: Missing / No / Yes)[156][157]. If “Yes,” then additional fields appear:
o	Type of thrombolytic (tPA vs TNK, etc.) – a dropdown if available in data, plus Missing.
o	IVT at external hospital? (ivt_different_hospital, yes/no/missing) – relevant if patient was transferred in after lytic.
o	Onset-to-IVT time (slider for minutes, if IVT was given)[158]. If IVT is No or Missing, these fields are disabled[159]. If IVT is Yes but time is missing, user can tick missing.
•	Transferred from another hospital? (transfer_from_other_hospital, yes/no) – separate from IVT, this is if patient started at another hospital. We include it since it might affect outcomes indirectly (in data it might correlate with delays).
•	Onset-to-puncture time (in minutes)[160] – slider for how many minutes from stroke onset to groin puncture (i.e., start of thrombectomy). This is a crucial predictor (shorter is better). If unknown, can mark missing (though in reality we almost always know it for treated patients).
•	Etiology: Stroke etiology (TOAST classification major groups, presumably). If our data had an etiology field with categories (e.g., Large artery, Cardioembolic, Other, Unknown), we provide a dropdown. If not in data, we skip. Based on structure, etiology is indeed listed in missing flags, so yes, we had it. So dropdown with categories plus Missing.
We designed the UI such that it’s intuitive and mirrors a real case form. The form also includes logic to handle dependencies: e.g., toggling IVT Yes/No will hide or show IVT-related fields (thrombolytic type, ivt_different_hospital, onset-to-IVT time)[157][161]. We do this using Streamlit’s ability to conditionally display parts of the form. Similarly, if “IVT info missing” is checked, we internally treat ivt_given as NaN and disable all IVT subfields.
For numeric inputs with missing checkboxes, our code likely sets the value to None if missing is checked, overriding whatever number is on the slider (this was implemented in helper functions).
10.4 Prediction and Explanation Output
Once the user has filled the form, they click a button like “Compute probability of good outcome (mRS 0–2)”[162]. The app then:
•	Builds the feature vector for this patient using a helper function (let’s call it build_patient_row)[163]. This function takes all the inputs and constructs a single-row DataFrame exactly in the format of mt_lightgbm_ready (all features, including all *_missing flags and interactions). Steps include:
•	Fill any missing main values with median (for numeric) or mode (for categorical) from training data only for the purpose of creating interaction features or any required downstream calculations. Actually, since our model can accept NaN for most features, we might not fill them for model input at all (LightGBM can handle them). But for safety, we might fill missing numeric with median so that when we compute interactions like age*NIHSS, we don’t get NaN * 10 = NaN. However, we can also design build_patient_row to compute interactions in a way that if any component is missing, the interaction is considered missing (which is effectively what the model would do if one part is NaN? Actually if either age or NIHSS is missing, age_x_nihss = NaN – and LightGBM handling is unclear, so better to impute for interactions at least).
•	Respect explicit “Missing” flags: for each feature that has a missing indicator, we ensure that if the user marked it missing, the value is set to NaN and the corresponding _missing flag is 1. If the user provided a value, the _missing flag is 0. All this is consistent with how training data was prepared.
•	Compute the interaction features from the provided values (with any needed median fill if one part is missing: e.g., if NIHSS is provided but age was missing – unusual scenario – we might fill age with median to compute ageNIHSS, though conceptually if age missing the model would rely on age_missing flag; maybe we set ageNIHSS to 0 or median*NIHSS; this is a corner case). Our approach likely: use median for missing age or NIHSS to compute the interaction (so as not to have NaN in the feature vector). Since missingness of age or NIHSS is already flagged, the model will know one of them was missing.
•	After building this row, we have a complete feature vector (with no NaNs if we chose to impute for interactions). For prediction, LightGBM can handle NaNs, but we likely imputed none-critical ones because LightGBM can treat NaN as a separate category effectively for each tree. It’s fine either way.
•	Predict probability: We feed this one-row DataFrame to the loaded mrs90_lgbm_model to get a probability[164]. Because we need to ensure the feature order matches training, we might use model.predict_proba(patient_df[model.feature_name_]) to align columns. The output is proba_good (and proba_bad = 1 - proba_good). We display these as percentages to the user: “Good outcome: 67%” and “Poor outcome: 33%” for example[165][166]. We typically highlight the probability of good outcome since that’s the positive class of interest.
•	Case-specific SHAP explanation: If SHAP is available (we ensure the SHAP package is present and our model, we have TreeExplainer which we can reuse since we computed global SHAP already, or we can instantiate a new one on the fly), we compute SHAP values for this single patient[167]. We likely use the TreeExplainer on the model (which is fast for one instance). This yields a SHAP value for each feature for this patient. We then:
•	Create a dataframe or list of features with their value and SHAP contribution. We sort it by absolute SHAP or so and take the top contributors.
•	Display a bar chart (horizontal bar chart likely) of the top ~5 or 10 features for this patient, showing how each one pushed the prediction up or down[168]. For example, it might show: “NIHSS = 20” with a big red bar to the left (negative effect), “age = 85” with a red bar left, “ASPECTS = 10” with a blue bar to right (positive effect), etc. The base value (expected outcome probability) might be around 50%, then adding all these contributions gets to the predicted 33%. We might label the base value as well.
•	We also display a table of the features and SHAP values in text, possibly.
•	This allows an experienced user to see why the model gave, say, 33%. E.g., “It predicted lower chance because the patient is very old and had a high NIHSS – those factors strongly reduce the probability. They did have a perfect ASPECTS, which helped a bit, but not enough to overcome the age and stroke severity.” This is exactly the kind of reasoning a clinician would do, and the model’s explanation aligns with it, thus increasing trust.
We ensure we clarify that SHAP contributions are in log-odds internally but we can also convert them to probability differences. We might just show magnitude though in the chart.
•	If the user checks a “Debug mode” (perhaps a checkbox for advanced users), we can show the full encoded feature vector behind the scenes[169]. This is mainly for us or data scientists to verify all features are set correctly. It might list every feature name and the numeric value that was fed to the model (including all the dummy missing flags and interactions). This can be hidden normally, but available if needed to troubleshoot.
Overall, the prediction output section likely has:
Probability of good outcome: 33% (big font)
Probability of poor outcome: 67%
Then maybe a color-coded gauge or progress bar.
Then an “Explanation” section:
- Table of top features:
1. NIHSS = 20 (this greatly lowers the chance of good outcome)
2. Age = 85 (lowers chance)
3. ASPECTS = 10 (raises chance slightly)
4. Onset-to-puncture = 90 min (slightly raises chance vs if it were longer)
5. etc…
And/or a bar chart illustrating this.
We explicitly mention the SHAP base value (e.g., “If this patient were an ‘average’ case, baseline chance of good outcome ~50%. Then the model subtracts 20% due to high NIHSS, subtracts 15% due to advanced age, adds 5% due to good ASPECTS, etc., arriving at ~33%.”).
For a clinician, this is like getting a second opinion that also explains itself: “Outcome not great, mainly because this is an elderly patient with a severe stroke, despite quick treatment.” That matches intuition, but the model quantifies it. In other cases with more nuance (maybe a middle-aged patient with moderate stroke but poor collaterals vs a very young with big stroke), the model’s weighing can sometimes provide insights into less obvious factors.
10.5 Embedded Performance Info
The app also has a section (perhaps an expandable sidebar or a bottom section) summarizing the model’s overall performance stats for reference[170][171]. We include:
•	Discrimination: We print the ROC AUC on the test set (e.g., “AUC = 0.77”), and explain in lay terms what that means: “If you randomly pick one patient who did well and one who did poorly, the model’s score was higher for the good-outcome patient 77% of the time”[171]. This gives context to accuracy.
•	Calibration: We mention the Brier score and especially the calibration slope & intercept[172][173]. But we translate that: e.g., “Calibration intercept = -0.05 (model slightly underestimates risk on average), slope = 0.94 (predictions are a bit too extreme; ideal would be 1.0).” Then we might add a sentence: “Overall, predicted probabilities are close to observed outcomes, but the model can be slightly overconfident for extreme predictions.” This is to caution the user if they see a probability like 95%, it might historically correspond to say 90% actual – still high, but not a guarantee.
•	Decision curve interpretation: We include a summary like we discussed: “The model shows positive net benefit in the threshold range ~15–40%. This suggests that if a clinician would take action only if the probability of good outcome is above some cutoff in that range, using the model to select patients yields more benefit than treating all or none. At very low thresholds, treating everyone yields similar outcomes (since almost everyone would be selected by the model anyway), and at very high thresholds, treating no one is similar (since model selects very few).”[174][175]. We might not include the full curve in the UI (though we could with streamlit charts), but the textual summary suffices to impart that the model is most useful for moderate-risk decisions.
•	Possibly mention cross-val: “In repeated cross-validation, the model’s AUC ranged from X to Y with mean ~0.77, indicating stable performance across different subsets.”
All this info is likely under an “Model Performance” expander that the user can click if interested, so it doesn’t clutter the main UI. It’s targeted at technically inclined clinicians or colleagues who want to know “how good is this model.”
By presenting these within the app, we maintain transparency. The clinician user is reminded: “This is not 100% accurate; here’s its measured accuracy.” That encourages appropriate caution in interpretation (especially since we emphasize it’s not for actual clinical use yet). It also educates on concepts like AUC and calibration in simple terms[171].
For example, we might have an expander titled “How accurate is this model?” and inside:
- “Discrimination (ROC AUC): 0.77. This means the model can correctly rank two patients (one good outcome, one poor) about 77% of the time. For context, an AUC of 1.0 is perfect, 0.5 is chance. So this model has good but not perfect discrimination.”[172]
- “Calibration: On the test set, when the model predicted X% risk, on average Y% of patients actually had good outcomes. The calibration intercept is close to 0 (–0.05) and slope is 0.94, indicating slight underestimation of probabilities and slight overconfidence. In practice, treat the probabilities as approximate. For example, a 50% predicted chance means roughly 50% observed frequency (which we consider well-calibrated).”
- “Net Benefit: The model’s decisions would add value if you would act on predictions above about 20% chance of good outcome. In that scenario, it can help identify patients who are more likely to benefit from certain interventions compared to treating all or none. If your threshold for action is too low or too high, the model doesn’t confer an advantage.”
- Possibly “Stability: The model’s performance was consistent when retrained on different subsets of data (cross-validation standard deviation of AUC ~0.03), suggesting it generalizes within this cohort.”
This level of detail is seldom present in typical apps, but given this is a research app, we included it to help interpret and to document limitations openly.
Finally, the app probably has disclaimers (we definitely would include some text like “Disclaimer: This is a research tool for demonstration only. Not for clinical use.” perhaps at the bottom or top). And after the performance info, maybe a note that external validation is pending, etc.
11. Summary
In conclusion, this repository delivers a fully reproducible pipeline for predicting 90-day outcomes after mechanical thrombectomy using only pre-treatment clinical data. To summarize:
•	We curated a high-quality dataset from a single-center thrombectomy registry, performing rigorous cleaning, validation, and documentation of data issues (ensuring the input data is reliable and well-understood).
•	We implemented a machine learning model (LightGBM) with monotonic constraints that embed clinical prior knowledge, improving interpretability. The model was tuned and evaluated extensively, showing decent discrimination (AUC in the high 0.70s) and good calibration for internal validation.
•	The model’s outputs were analyzed for clinical relevance: it appears to be well-calibrated and potentially useful in guiding decisions within a certain risk range, though it’s not a definitive predictor. Key prognostic factors like age, stroke severity, and treatment time drive the predictions, as expected.
•	All aspects of the model development are transparently reported (TRIPOD): we provide detailed logs of data processing, full specification of the model features and parameters, and all performance metrics, enabling scrutiny and reproducibility.
•	A password-protected Streamlit app is included, which allows interactive exploration of the model. Clinicians can input patient data to see the predicted outcome probability and, importantly, an explanation of the prediction via SHAP (identifying which factors contribute to that patient’s prognosis). The app also communicates the model’s overall accuracy and limitations in an accessible way, making it a useful demonstration tool for discussions with clinical teams or for internal validation with new cases.
Future steps and limitations: Before any real clinical deployment, the model needs external validation on data from other centers to ensure it generalizes to different populations and practices[176]. If it underperforms or is miscalibrated externally, adjustments or recalibration (e.g., updating the logistic calibration intercept) may be required[177]. The model currently doesn’t incorporate intra-procedural or post-procedural factors – by design, to focus on pre-procedure prognostication[178]. This means it can’t predict the modified Rankin if a complication happens during thrombectomy, etc. But it aligns with use at admission or pre-treatment counseling. We also have not conducted subgroup analyses beyond age – in the future, one might examine if the model is equally valid in, say, men vs women, or patients with versus without tandem occlusions (ensuring fairness and identifying if further interaction terms should be added). Additionally, outcome definition here is mRS 0–2; for other outcomes (like excellent outcome 0–1, or ordinal mRS shift), new models would be needed.
Nevertheless, this work demonstrates a blueprint for developing an explainable machine learning model in stroke: we’ve shown that using pre-treatment data we can reasonably predict outcomes, quantify uncertainty, and present the information in a user-friendly manner. The combination of performance metrics, decision analysis, and explainability techniques (SHAP) in one package is quite powerful for translating the model’s insights to clinicians. The emphasis on monotonic relationships and thorough validation addresses some common criticisms of ML models (black-box nature, lack of trust).
This project can be extended or adapted as more data becomes available (for example, retraining or updating with new patients, adding imaging biomarkers like collateral scores, etc.). The modular pipeline structure (thanks to Kedro) makes it easy to plug in new steps or run what-if analyses (like what if we drop a certain feature, etc., via new pipeline branches).
Finally, while the current Streamlit app is mainly for research demonstration, it lays the groundwork for a potential clinical decision support tool (pending the validations and regulatory approvals). With appropriate safeguards, such a tool could one day help inform doctors and patients about likely outcomes early in the treatment course, aiding in setting expectations and making management decisions (for instance, intensive rehab vs palliative approach for very poor prognosis patients – though such decisions are complex and multifactorial, the model would only ever supplement, not replace, clinical judgment).
In summary, MTOP (Mechanical Thrombectomy Outcome Predictor) provides:
•	A reproducible pipeline turning raw clinical data into a trained prognostic model.
•	Comprehensive internal validation demonstrating how the model performs and how it reaches its predictions.
•	A user interface for practical demonstration of the model’s capabilities and limitations.
This ensures that anyone reviewing or using the model can understand exactly what it’s doing and why, which is essential for building confidence in its predictions. The project will continue to evolve as we gather external data and feedback, moving toward a robust tool for outcome prediction in stroke thrombectomy patients.
